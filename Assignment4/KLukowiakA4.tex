\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Assignment 4},
            pdfauthor={Kai Lukowiak},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Assignment 4}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Kai Lukowiak}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{2018-03-25}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage[normalem]{ulem}

\begin{document}
\maketitle
\begin{abstract}
This paper looks into the predictive ability of certain factors into the
likelyhood of a person getting into an accident and also the amount that
the accident will cost.
\end{abstract}

{
\setcounter{tocdepth}{3}
\tableofcontents
}
\hypertarget{data-exploration}{%
\section{Data Exploration}\label{data-exploration}}

\hypertarget{the-data-frames}{%
\subsection{The Data Frames}\label{the-data-frames}}

\begin{longtable}[]{@{}rrrrrrrlrllllrlrrllrrlrrl@{}}
\toprule
TARGET\_FLAG & TARGET\_AMT & KIDSDRIV & AGE & HOMEKIDS & YOJ & INCOME &
PARENT1 & HOME\_VAL & MSTATUS & SEX & EDUCATION & JOB & TRAVTIME &
CAR\_USE & BLUEBOOK & TIF & CAR\_TYPE & RED\_CAR & OLDCLAIM & CLM\_FREQ
& REVOKED & MVR\_PTS & CAR\_AGE & URBANICITY\tabularnewline
\midrule
\endhead
0 & 0.000 & 0 & 30 & 2 & 13 & 103881 & No & 287047 & Yes & M & Bachelors
& z\_Blue Collar & 33 & Commercial & 14890 & 1 & Minivan & no & 0 & 0 &
No & 1 & 9 & z\_Highly Rural/ Rural\tabularnewline
1 & 2044.217 & 0 & 33 & 0 & 9 & 69641 & No & 230290 & z\_No & M &
Bachelors & z\_Blue Collar & 33 & Commercial & 22680 & 1 & Panel Truck &
yes & 1712 & 4 & No & 2 & 15 & Highly Urban/ Urban\tabularnewline
0 & 0.000 & 0 & 46 & 0 & 12 & 88359 & No & 267679 & Yes & z\_F & Masters
& Lawyer & 25 & Private & 31980 & 3 & Minivan & no & 0 & 0 & Yes & 0 & 1
& Highly Urban/ Urban\tabularnewline
0 & 0.000 & 0 & 27 & 3 & 7 & 131567 & Yes & 0 & z\_No & z\_F & Bachelors
& Professional & 32 & Private & 8790 & 1 & Minivan & no & 0 & 0 & No & 3
& 1 & Highly Urban/ Urban\tabularnewline
0 & 0.000 & 0 & 51 & 0 & 13 & 85345 & No & NA & Yes & M & Masters & NA &
37 & Commercial & 34270 & 4 & Panel Truck & no & 8481 & 3 & No & 5 & 15
& Highly Urban/ Urban\tabularnewline
0 & 0.000 & 0 & 35 & 1 & 10 & 16039 & No & 124191 & Yes & z\_F & z\_High
School & Clerical & 5 & Private & 4010 & 4 & z\_SUV & no & 38690 & 2 &
No & 3 & 10 & Highly Urban/ Urban\tabularnewline
\bottomrule
\end{longtable}

\begin{verbatim}
## Observations: 6,121
## Variables: 25
## $ TARGET_FLAG <int> 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,...
## $ TARGET_AMT  <dbl> 0.000, 0.000, 0.000, 0.000, 0.000, 2946.000, 0.000...
## $ KIDSDRIV    <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ AGE         <int> 60, 43, 35, 51, 50, 34, 54, 37, 34, 50, 53, 43, 55...
## $ HOMEKIDS    <int> 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3,...
## $ YOJ         <int> 11, 11, 10, 14, NA, 12, NA, NA, 10, 7, 14, 5, 11, ...
## $ INCOME      <dbl> 67349, 91449, 16039, NA, 114986, 125301, 18755, 10...
## $ PARENT1     <fct> No, No, No, No, No, Yes, No, No, No, No, No, No, N...
## $ HOME_VAL    <dbl> 0, 257252, 124191, 306251, 243925, 0, NA, 333680, ...
## $ MSTATUS     <fct> z_No, z_No, Yes, Yes, Yes, z_No, Yes, Yes, z_No, z...
## $ SEX         <fct> M, M, z_F, M, z_F, z_F, z_F, M, z_F, M, z_F, z_F, ...
## $ EDUCATION   <fct> PhD, z_High School, z_High School, <High School, P...
## $ JOB         <fct> Professional, z_Blue Collar, Clerical, z_Blue Coll...
## $ TRAVTIME    <int> 14, 22, 5, 32, 36, 46, 33, 44, 34, 48, 15, 36, 25,...
## $ CAR_USE     <fct> Private, Commercial, Private, Private, Private, Co...
## $ BLUEBOOK    <dbl> 14230, 14940, 4010, 15440, 18000, 17430, 8780, 169...
## $ TIF         <int> 11, 1, 4, 7, 1, 1, 1, 1, 1, 7, 1, 7, 7, 6, 1, 6, 7...
## $ CAR_TYPE    <fct> Minivan, Minivan, z_SUV, Minivan, z_SUV, Sports Ca...
## $ RED_CAR     <fct> yes, yes, no, yes, no, no, no, yes, no, no, no, no...
## $ OLDCLAIM    <dbl> 4461, 0, 38690, 0, 19217, 0, 0, 2374, 0, 0, 0, 0, ...
## $ CLM_FREQ    <int> 2, 0, 2, 0, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0,...
## $ REVOKED     <fct> No, No, No, No, Yes, No, No, Yes, No, No, No, No, ...
## $ MVR_PTS     <int> 3, 0, 3, 0, 3, 0, 0, 10, 0, 1, 0, 0, 3, 3, 3, 0, 0...
## $ CAR_AGE     <int> 18, 1, 10, 6, 17, 7, 1, 7, 1, 17, 11, 1, 9, 10, 5,...
## $ URBANICITY  <fct> Highly Urban/ Urban, Highly Urban/ Urban, Highly U...
\end{verbatim}

The trainind dataset is comprised of 26 variables, two of which are
response variables, \texttt{TARGET\_FLAG} and \texttt{TARGET\_AMT}.
These will be used to run logistic and regular regression respectivly.

The evaluation set looks similar but has \texttt{NA}s in the first two
rows.

\begin{longtable}[]{@{}rrrrrlrl@{}}
\caption{Sample of Values for the Test Set}\tabularnewline
\toprule
KIDSDRIV & AGE & HOMEKIDS & YOJ & INCOME & PARENT1 & HOME\_VAL &
MSTATUS\tabularnewline
\midrule
\endfirsthead
\toprule
KIDSDRIV & AGE & HOMEKIDS & YOJ & INCOME & PARENT1 & HOME\_VAL &
MSTATUS\tabularnewline
\midrule
\endhead
0 & 47 & 0 & 12 & 36150 & No & 174964 & Yes\tabularnewline
0 & 57 & 0 & 10 & 130831 & No & 365792 & Yes\tabularnewline
0 & 50 & 0 & 14 & 15989 & No & 117038 & Yes\tabularnewline
0 & 47 & 0 & 14 & 2825 & No & 91520 & Yes\tabularnewline
0 & 58 & 0 & 12 & 73283 & No & 251025 & Yes\tabularnewline
0 & 39 & 3 & 14 & 51524 & Yes & 0 & z\_No\tabularnewline
\bottomrule
\end{longtable}

The evaluation set is a similar data frame but excludes the target
variable. As such it cannot be used for cross validation.

\begin{verbatim}
## Parsed with column specification:
## cols(
##   `Variable Name` = col_character(),
##   Definition = col_character(),
##   `Theoretical Effect` = col_character()
## )
\end{verbatim}

\begin{longtable}[]{@{}lll@{}}
\toprule
Variable Name & Definition & Theoretical Effect\tabularnewline
\midrule
\endhead
INDEX & Index & None\tabularnewline
TARGET\_FLAG & Identification Variable (do not use) &
None\tabularnewline
TARGET\_AMT & Was Car in crash 1=YES 0=NO & None\tabularnewline
AGE & Age of Driver & Very young people tend to be risky. Maybe very old
people also.\tabularnewline
BLUEBOOK & Value of Vehicle & Unknown effect on probability of
collision, but probably effect the payout if there is a
crash\tabularnewline
CAR\_AGE & Vehicle Age & Unknown effect on probability of collision, but
probably effect the payout if there is a crash\tabularnewline
CAR\_TYPE & Type of Car & Unknown effect on probability of collision,
but probably effect the payout if there is a crash\tabularnewline
CAR\_USE & Vehicle Use & Commercial vehicles are driven more, so might
increase probability of collision\tabularnewline
CLM\_FREQ & \# Claims (Past 5 Years) & The more claims you filed in the
past, the more you are likely to file in the future\tabularnewline
EDUCATION & Max Education Level & Unknown effect, but in theory more
educated people tend to drive more safely\tabularnewline
HOMEKIDS & \# Children at Home & Unknown effect\tabularnewline
HOME\_VAL & Home Value & In theory, home owners tend to drive more
responsibly\tabularnewline
INCOME & Income & In theory, rich people tend to get into fewer
crashes\tabularnewline
JOB & Job Category & In theory, white collar jobs tend to be
safer\tabularnewline
KIDSDRIV & \# Driving Children & When teenagers drive your car, you are
more likely to get into crashes\tabularnewline
MSTATUS & Matitial Status & In theory, married people drive more
safely\tabularnewline
MVR\_PTS & Motor Vehicle Record Points & If you get lots of traffic
tickets, you tend to get into more crashes\tabularnewline
OLDCLAIM & Total Claims (Past 5 Years) & If your total payout over the
past five years was high, this suggests future payouts will be
high\tabularnewline
PARENT1 & Single Parent & Unknown effect\tabularnewline
RED\_CAR & A red Car & Urban legend says that red cars (especially red
sports cars) are more risky. Is that true?\tabularnewline
REVOKED & License Revoked (Past 7 Years) & If your license was revoked
in the past 7 years, you probably are a more risky
driver.\tabularnewline
Sex & Time in Force & Urban legend says that women have less crashes
then men. Is that true?\tabularnewline
TIF & Time in Force & People who have been customers for a long time are
usually more safe.\tabularnewline
TRAVTIME & Distance to Work & Long drives to work usually suggest
greater risk\tabularnewline
URBANCITY & Home/Work Area & Unknown\tabularnewline
YOJ & Years on Job & People who stay at a job for a long time are
usually more safe\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{summary-statistics}{%
\subsection{Summary Statistics}\label{summary-statistics}}

\begin{longtable}[]{@{}lcccccc@{}}
\caption{Summary Statistics}\tabularnewline
\toprule
& TARGET\_FLAG & TARGET\_AMT & KIDSDRIV & AGE & HOMEKIDS &
YOJ\tabularnewline
\midrule
\endfirsthead
\toprule
& TARGET\_FLAG & TARGET\_AMT & KIDSDRIV & AGE & HOMEKIDS &
YOJ\tabularnewline
\midrule
\endhead
& Min. :0.0000 & Min. : 0 & Min. :0.0000 & Min. :16.00 & Min. :0.0000 &
Min. : 0.00\tabularnewline
& 1st Qu.:0.0000 & 1st Qu.: 0 & 1st Qu.:0.0000 & 1st Qu.:39.00 & 1st
Qu.:0.0000 & 1st Qu.: 9.00\tabularnewline
& Median :0.0000 & Median : 0 & Median :0.0000 & Median :45.00 & Median
:0.0000 & Median :11.00\tabularnewline
& Mean :0.2638 & Mean : 1514 & Mean :0.1704 & Mean :44.68 & Mean :0.7298
& Mean :10.53\tabularnewline
& 3rd Qu.:1.0000 & 3rd Qu.: 1029 & 3rd Qu.:0.0000 & 3rd Qu.:51.00 & 3rd
Qu.:1.0000 & 3rd Qu.:13.00\tabularnewline
& Max. :1.0000 & Max. :107586 & Max. :4.0000 & Max. :73.00 & Max.
:5.0000 & Max. :19.00\tabularnewline
& NA & NA & NA & NA's :3 & NA & NA's :347\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lclclccc@{}}
\caption{Summary Statistics}\tabularnewline
\toprule
& INCOME & PARENT1 & HOME\_VAL & MSTATUS & SEX & EDUCATION &
JOB\tabularnewline
\midrule
\endfirsthead
\toprule
& INCOME & PARENT1 & HOME\_VAL & MSTATUS & SEX & EDUCATION &
JOB\tabularnewline
\midrule
\endhead
& Min. : 0 & No :5289 & Min. : 0 & Yes :3659 & M :2827 & \textless{}High
School : 933 & z\_Blue Collar:1375\tabularnewline
& 1st Qu.: 27658 & Yes: 832 & 1st Qu.: 0 & z\_No:2462 & z\_F:3294 &
Bachelors :1693 & Clerical : 961\tabularnewline
& Median : 53904 & NA & Median :159417 & NA & NA & Masters :1240 &
Professional : 834\tabularnewline
& Mean : 61890 & NA & Mean :154164 & NA & NA & PhD : 541 & Manager :
738\tabularnewline
& 3rd Qu.: 86464 & NA & 3rd Qu.:238931 & NA & NA & z\_High School:1714 &
Lawyer : 615\tabularnewline
& Max. :367030 & NA & Max. :885282 & NA & NA & NA & (Other)
:1191\tabularnewline
& NA's :343 & NA & NA's :344 & NA & NA & NA & NA's : 407\tabularnewline
\bottomrule
\end{longtable}

These tables give an overview of the variables, suggesting there may be
some issues with distributions but we will need to look further before
making any decisions on transforming the variables.

\hypertarget{descriptive-statistics}{%
\subsection{Descriptive Statistics}\label{descriptive-statistics}}

\begin{longtable}[]{@{}lrrrrr@{}}
\caption{Descriptive Statistics}\tabularnewline
\toprule
& vars & n & mean & sd & median\tabularnewline
\midrule
\endfirsthead
\toprule
& vars & n & mean & sd & median\tabularnewline
\midrule
\endhead
TARGET\_FLAG & 1 & 6121 & 2.638458e-01 & 4.407527e-01 &
0.0\tabularnewline
TARGET\_AMT & 2 & 6121 & 1.514009e+03 & 4.776983e+03 &
0.0\tabularnewline
KIDSDRIV & 3 & 6121 & 1.703970e-01 & 5.102345e-01 & 0.0\tabularnewline
AGE & 4 & 6118 & 4.467980e+01 & 8.647718e+00 & 45.0\tabularnewline
HOMEKIDS & 5 & 6121 & 7.297827e-01 & 1.122196e+00 & 0.0\tabularnewline
YOJ & 6 & 5774 & 1.053152e+01 & 4.084161e+00 & 11.0\tabularnewline
INCOME & 7 & 5778 & 6.188946e+04 & 4.785552e+04 & 53903.5\tabularnewline
PARENT1* & 8 & 6121 & 1.135925e+00 & 3.427374e-01 & 1.0\tabularnewline
HOME\_VAL & 9 & 5777 & 1.541642e+05 & 1.303360e+05 &
159417.0\tabularnewline
MSTATUS* & 10 & 6121 & 1.402222e+00 & 4.903863e-01 & 1.0\tabularnewline
SEX* & 11 & 6121 & 1.538147e+00 & 4.985834e-01 & 2.0\tabularnewline
EDUCATION* & 12 & 6121 & 3.066982e+00 & 1.445894e+00 &
3.0\tabularnewline
JOB* & 13 & 5714 & 5.008050e+00 & 2.468908e+00 & 5.0\tabularnewline
TRAVTIME & 14 & 6121 & 3.347100e+01 & 1.593407e+01 & 33.0\tabularnewline
CAR\_USE* & 15 & 6121 & 1.627022e+00 & 4.836359e-01 & 2.0\tabularnewline
BLUEBOOK & 16 & 6121 & 1.567134e+04 & 8.455444e+03 &
14380.0\tabularnewline
TIF & 17 & 6121 & 5.349616e+00 & 4.106822e+00 & 4.0\tabularnewline
CAR\_TYPE* & 18 & 6121 & 3.521157e+00 & 1.960455e+00 &
3.0\tabularnewline
RED\_CAR* & 19 & 6121 & 1.290802e+00 & 4.541695e-01 & 1.0\tabularnewline
OLDCLAIM & 20 & 6121 & 4.070660e+03 & 8.867756e+03 & 0.0\tabularnewline
CLM\_FREQ & 21 & 6121 & 7.952949e-01 & 1.152753e+00 & 0.0\tabularnewline
REVOKED* & 22 & 6121 & 1.121549e+00 & 3.267906e-01 & 1.0\tabularnewline
MVR\_PTS & 23 & 6121 & 1.719164e+00 & 2.150401e+00 & 1.0\tabularnewline
CAR\_AGE & 24 & 5735 & 8.334612e+00 & 5.747126e+00 & 8.0\tabularnewline
URBANICITY* & 25 & 6121 & 1.207156e+00 & 4.053012e-01 &
1.0\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lrrrr@{}}
\caption{Descriptive Statistics}\tabularnewline
\toprule
& trimmed & mad & min & max\tabularnewline
\midrule
\endfirsthead
\toprule
& trimmed & mad & min & max\tabularnewline
\midrule
\endhead
TARGET\_FLAG & 2.048193e-01 & 0.0000 & 0 & 1.0\tabularnewline
TARGET\_AMT & 5.889851e+02 & 0.0000 & 0 & 107586.1\tabularnewline
KIDSDRIV & 2.470900e-02 & 0.0000 & 0 & 4.0\tabularnewline
AGE & 4.473284e+01 & 8.8956 & 16 & 73.0\tabularnewline
HOMEKIDS & 5.064325e-01 & 0.0000 & 0 & 5.0\tabularnewline
YOJ & 1.110736e+01 & 2.9652 & 0 & 19.0\tabularnewline
INCOME & 5.676326e+04 & 42610.6653 & 0 & 367030.0\tabularnewline
PARENT1* & 1.044925e+00 & 0.0000 & 1 & 2.0\tabularnewline
HOME\_VAL & 1.428207e+05 & 152887.1946 & 0 & 885282.0\tabularnewline
MSTATUS* & 1.377782e+00 & 0.0000 & 1 & 2.0\tabularnewline
SEX* & 1.547682e+00 & 0.0000 & 1 & 2.0\tabularnewline
EDUCATION* & 3.083725e+00 & 1.4826 & 1 & 5.0\tabularnewline
JOB* & 5.134952e+00 & 2.9652 & 1 & 8.0\tabularnewline
TRAVTIME & 3.301062e+01 & 16.3086 & 5 & 142.0\tabularnewline
CAR\_USE* & 1.658771e+00 & 0.0000 & 1 & 2.0\tabularnewline
BLUEBOOK & 1.498274e+04 & 8510.1240 & 1500 & 69740.0\tabularnewline
TIF & 4.854605e+00 & 4.4478 & 1 & 25.0\tabularnewline
CAR\_TYPE* & 3.526445e+00 & 2.9652 & 1 & 6.0\tabularnewline
RED\_CAR* & 1.238513e+00 & 0.0000 & 1 & 2.0\tabularnewline
OLDCLAIM & 1.727992e+03 & 0.0000 & 0 & 57037.0\tabularnewline
CLM\_FREQ & 5.860731e-01 & 0.0000 & 0 & 5.0\tabularnewline
REVOKED* & 1.026955e+00 & 0.0000 & 1 & 2.0\tabularnewline
MVR\_PTS & 1.341842e+00 & 1.4826 & 0 & 13.0\tabularnewline
CAR\_AGE & 7.954892e+00 & 7.4130 & 0 & 28.0\tabularnewline
URBANICITY* & 1.133960e+00 & 0.0000 & 1 & 2.0\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lrrrr@{}}
\caption{Descriptive Statistics}\tabularnewline
\toprule
& range & skew & kurtosis & se\tabularnewline
\midrule
\endfirsthead
\toprule
& range & skew & kurtosis & se\tabularnewline
\midrule
\endhead
TARGET\_FLAG & 1.0 & 1.0714201 & -0.8521981 & 0.0056336\tabularnewline
TARGET\_AMT & 107586.1 & 8.7901683 & 115.2132560 &
61.0579864\tabularnewline
KIDSDRIV & 4.0 & 3.3505408 & 11.7603038 & 0.0065217\tabularnewline
AGE & 57.0 & -0.0564481 & -0.0854251 & 0.1105597\tabularnewline
HOMEKIDS & 5.0 & 1.3343569 & 0.6505442 & 0.0143436\tabularnewline
YOJ & 19.0 & -1.2180676 & 1.1808636 & 0.0537483\tabularnewline
INCOME & 367030.0 & 1.2145580 & 2.2994723 & 629.5688594\tabularnewline
PARENT1* & 1.0 & 2.1241626 & 2.5124772 & 0.0043808\tabularnewline
HOME\_VAL & 885282.0 & 0.5294461 & 0.0815993 &
1714.7992717\tabularnewline
MSTATUS* & 1.0 & 0.3987149 & -1.8413272 & 0.0062680\tabularnewline
SEX* & 1.0 & -0.1529980 & -1.9769145 & 0.0063727\tabularnewline
EDUCATION* & 4.0 & 0.1363664 & -1.3715543 & 0.0184810\tabularnewline
JOB* & 7.0 & -0.3418602 & -1.1629758 & 0.0326614\tabularnewline
TRAVTIME & 137.0 & 0.4428075 & 0.7156313 & 0.2036646\tabularnewline
CAR\_USE* & 1.0 & -0.5251925 & -1.7244545 & 0.0061817\tabularnewline
BLUEBOOK & 68240.0 & 0.8097543 & 0.8315495 & 108.0749994\tabularnewline
TIF & 24.0 & 0.8608451 & 0.3166601 & 0.0524922\tabularnewline
CAR\_TYPE* & 5.0 & 0.0052126 & -1.5099694 & 0.0250580\tabularnewline
RED\_CAR* & 1.0 & 0.9210819 & -1.1517962 & 0.0058051\tabularnewline
OLDCLAIM & 57037.0 & 3.1274684 & 9.9061482 & 113.3450445\tabularnewline
CLM\_FREQ & 5.0 & 1.2115436 & 0.3061895 & 0.0147342\tabularnewline
REVOKED* & 1.0 & 2.3157911 & 3.3634379 & 0.0041769\tabularnewline
MVR\_PTS & 13.0 & 1.3131263 & 1.2250866 & 0.0274858\tabularnewline
CAR\_AGE & 28.0 & 0.2984052 & -0.7372102 & 0.0758899\tabularnewline
URBANICITY* & 1.0 & 1.4448340 & 0.0875597 & 0.0051804\tabularnewline
\bottomrule
\end{longtable}

The count of NA values for each variable is given below.

\begin{table}
\caption{Count of NA Values}

\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
3\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
347\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
343\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\end{table}

\begin{table}
\caption{Count of NA Values}

\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
344\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
407\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\end{table}

\begin{table}
\caption{Count of NA Values}

\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
386\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0\\
\hline
\end{tabular}
\end{table}

There are quite a few missing values accross several variables. However,
compared to the size of the training set, around 6000, these numbers
could be dropped if there is no correlation between the missing values
and the response variables.

The correlation between missing values and the `Claim Filed' response is
-3.1516899\times 10\^{}\{-4\} and 0.0086636 for the claim amount. Since
these are very close to zero we are not worried about them effecting the
regressions. As such, we will drop them.

\hypertarget{graphical-eda}{%
\subsection{Graphical EDA}\label{graphical-eda}}

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-8-1.pdf}

The distrobutions are generally skeded upwards, nothing suggests
problems with the dataset. The only variable that is very skewed is
\texttt{TARGET\_AMT} and this makes sense because most are zero or low
and some are very high.

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-9-1.pdf}

From these two graphs we can see that many of distributions are skewed
in one direction or another. It is also interesting to see that the
target variable is below zero. This means that the median and mean
values are different.

\begin{verbatim}
## Warning: attributes are not identical across measure variables;
## they will be dropped
\end{verbatim}

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-10-1.pdf}

The factor variables some uneven counts as well but nothing that is
highly out of the ordinary.

\hypertarget{data-preperation}{%
\section{Data Preperation}\label{data-preperation}}

\hypertarget{transformed-skewed-variables}{%
\subsection{Transformed Skewed
Variables}\label{transformed-skewed-variables}}

I will log transform \texttt{TARGET\_AMT} in one of the models that
build to account for the wide range. During this transformation it is
important to add 1 to each variable because there are many zero values
that would throw and error.

I also transformed \texttt{TARGET\_AMT} with the power of -0.4 based off
of a Box-Cox analysis. Given the nature of other variables it does not
seem necessary to transform others.

\hypertarget{build-models}{%
\section{Build Models}\label{build-models}}

Model selection will be based off of automated selection techniques as
well as specific transformations.

\hypertarget{ordinary-least-squares}{%
\subsection{Ordinary Least Squares}\label{ordinary-least-squares}}

\hypertarget{baisic-regression}{%
\subsubsection{Baisic Regression}\label{baisic-regression}}

Here I regressed all variables without transformation.

Call: lm(formula = TARGET\_AMT \textasciitilde{} ., data = dfCont)

Residuals: Min 1Q Median 3Q Max -5602 -1702 -733 419 82515

Coefficients: Estimate Std. Error t value
Pr(\textgreater{}\textbar{}t\textbar{})\\
(Intercept) 2.206e+03 6.267e+02 3.520 0.000436 \textbf{\emph{ KIDSDRIV
2.624e+02 1.487e+02 1.765 0.077705 .\\
AGE -5.843e+00 9.387e+00 -0.622 0.533648\\
HOMEKIDS -3.457e+01 8.506e+01 -0.406 0.684471\\
YOJ -8.626e+00 1.944e+01 -0.444 0.657279\\
INCOME -2.588e-03 2.719e-03 -0.952 0.341152\\
PARENT1Yes 6.711e+02 2.643e+02 2.539 0.011153 }\\
HOME\_VAL -1.373e-03 8.504e-04 -1.615 0.106471\\
MSTATUSz\_No 4.870e+02 2.001e+02 2.434 0.014982 *\\
SEXz\_F -3.404e+02 2.408e+02 -1.414 0.157515\\
EDUCATIONBachelors -3.255e+02 2.662e+02 -1.222 0.221603\\
EDUCATIONMasters -3.557e+02 4.018e+02 -0.885 0.376129\\
EDUCATIONPhD 5.905e+02 4.993e+02 1.183 0.236960\\
EDUCATIONz\_High School -1.534e+02 2.200e+02 -0.697 0.485834\\
JOBDoctor -1.365e+03 5.902e+02 -2.313 0.020762 *\\
JOBHome Maker 1.498e+01 3.263e+02 0.046 0.963375\\
JOBLawyer 1.771e+02 4.001e+02 0.443 0.658116\\
JOBManager -9.372e+02 3.075e+02 -3.048 0.002320 } JOBProfessional
2.831e+02 2.825e+02 1.002 0.316268\\
JOBStudent -2.023e+02 3.145e+02 -0.643 0.520122\\
JOBz\_Blue Collar 8.733e+01 2.464e+02 0.354 0.723062\\
TRAVTIME 1.666e+01 4.277e+00 3.895 9.98e-05 \textbf{\emph{
CAR\_USEPrivate -8.516e+02 2.169e+02 -3.926 8.76e-05 }} BLUEBOOK
1.124e-02 1.139e-02 0.987 0.323848\\
TIF -4.546e+01 1.639e+01 -2.773 0.005570 ** CAR\_TYPEPanel Truck
3.648e+02 3.928e+02 0.929 0.353128\\
CAR\_TYPEPickup 4.390e+02 2.222e+02 1.975 0.048285 *\\
CAR\_TYPESports Car 1.304e+03 2.773e+02 4.704 2.63e-06 \textbf{\emph{
CAR\_TYPEVan 5.659e+02 2.912e+02 1.943 0.052050 .\\
CAR\_TYPEz\_SUV 7.651e+02 2.299e+02 3.328 0.000881 }} RED\_CARyes
-1.374e+02 2.034e+02 -0.676 0.499234\\
OLDCLAIM -7.776e-03 9.851e-03 -0.789 0.429963\\
CLM\_FREQ 5.019e+01 7.386e+01 0.680 0.496839\\
REVOKEDYes 3.721e+02 2.356e+02 1.580 0.114251\\
MVR\_PTS 1.654e+02 3.434e+01 4.817 1.51e-06 \textbf{\emph{ CAR\_AGE
-2.414e+01 1.720e+01 -1.404 0.160504\\
URBANICITYz\_Highly Rural/ Rural -1.918e+03 1.814e+02 -10.573
\textless{} 2e-16 }} --- Signif. codes: 0 `\emph{\textbf{' 0.001 '}'
0.01 '}' 0.05 `.' 0.1 `' 1

Residual standard error: 4494 on 4468 degrees of freedom Multiple
R-squared: 0.07951, Adjusted R-squared: 0.07209 F-statistic: 10.72 on 36
and 4468 DF, p-value: \textless{} 2.2e-16

This is a pretty poor \(R^2\). While there are some significant
variables, the overall performance is poor.

\hypertarget{log-transformed}{%
\subsubsection{Log Transformed}\label{log-transformed}}

Next we look at the regression with a log transformed
\texttt{TARGET\_AMT} variable.

Call: lm(formula = log(TARGET\_AMT + 1) \textasciitilde{} ., data =
dfCont)

Residuals: Min 1Q Median 3Q Max -8.173 -2.340 -0.850 2.146 11.054

Coefficients: Estimate Std. Error t value
Pr(\textgreater{}\textbar{}t\textbar{})\\
(Intercept) 3.640e+00 4.466e-01 8.150 4.66e-16 \textbf{\emph{ KIDSDRIV
3.949e-01 1.060e-01 3.727 0.000197 }} AGE -6.131e-03 6.689e-03 -0.916
0.359457\\
HOMEKIDS -2.705e-02 6.061e-02 -0.446 0.655468\\
YOJ -1.264e-02 1.385e-02 -0.912 0.361683\\
INCOME -3.189e-06 1.937e-06 -1.646 0.099790 .\\
PARENT1Yes 8.902e-01 1.884e-01 4.726 2.36e-06 \textbf{\emph{ HOME\_VAL
-1.442e-06 6.060e-07 -2.379 0.017379 }\\
MSTATUSz\_No 4.756e-01 1.426e-01 3.335 0.000860 }\emph{ SEXz\_F
-1.547e-01 1.716e-01 -0.902 0.367280\\
EDUCATIONBachelors -5.226e-01 1.897e-01 -2.755 0.005901 \textbf{
EDUCATIONMasters -4.859e-01 2.864e-01 -1.697 0.089781 .\\
EDUCATIONPhD 1.498e-01 3.558e-01 0.421 0.673785\\
EDUCATIONz\_High School 4.030e-02 1.568e-01 0.257 0.797156\\
JOBDoctor -1.388e+00 4.206e-01 -3.299 0.000978 }} JOBHome Maker
-3.566e-01 2.325e-01 -1.534 0.125162\\
JOBLawyer -2.596e-01 2.851e-01 -0.910 0.362667\\
JOBManager -1.395e+00 2.191e-01 -6.368 2.11e-10 \textbf{\emph{
JOBProfessional -4.193e-01 2.013e-01 -2.083 0.037331 }\\
JOBStudent -4.121e-01 2.241e-01 -1.839 0.065963 .\\
JOBz\_Blue Collar -3.573e-01 1.756e-01 -2.035 0.041932 *\\
TRAVTIME 2.163e-02 3.048e-03 7.095 1.50e-12 }\emph{ CAR\_USEPrivate
-1.199e+00 1.546e-01 -7.754 1.10e-14 }\textbf{ BLUEBOOK -2.055e-05
8.115e-06 -2.532 0.011368 *\\
TIF -6.686e-02 1.168e-02 -5.724 1.11e-08 }\emph{ CAR\_TYPEPanel Truck
5.446e-01 2.800e-01 1.945 0.051794 .\\
CAR\_TYPEPickup 6.586e-01 1.584e-01 4.159 3.26e-05 }\textbf{
CAR\_TYPESports Car 1.332e+00 1.976e-01 6.740 1.78e-11 }\emph{
CAR\_TYPEVan 5.699e-01 2.075e-01 2.746 0.006057 \textbf{ CAR\_TYPEz\_SUV
8.588e-01 1.638e-01 5.242 1.66e-07 }} RED\_CARyes -2.352e-01 1.449e-01
-1.623 0.104640\\
OLDCLAIM -1.883e-05 7.020e-06 -2.682 0.007347 ** CLM\_FREQ 2.477e-01
5.263e-02 4.706 2.60e-06 \textbf{\emph{ REVOKEDYes 1.058e+00 1.679e-01
6.300 3.27e-10 }} MVR\_PTS 1.949e-01 2.447e-02 7.965 2.07e-15
\textbf{\emph{ CAR\_AGE -8.615e-03 1.226e-02 -0.703 0.482150\\
URBANICITYz\_Highly Rural/ Rural -2.571e+00 1.293e-01 -19.894
\textless{} 2e-16 }} --- Signif. codes: 0 `\emph{\textbf{' 0.001 '}'
0.01 '}' 0.05 `.' 0.1 `' 1

Residual standard error: 3.203 on 4468 degrees of freedom Multiple
R-squared: 0.2429, Adjusted R-squared: 0.2368 F-statistic: 39.81 on 36
and 4468 DF, p-value: \textless{} 2.2e-16

This is an improvement. It makes sense that we would need to transform
the response variable given it's skewed nature.

\hypertarget{boxcox}{%
\subsection{BoxCox}\label{boxcox}}

bcPower Transformation to Normality Est Power Rounded Pwr Wald Lwr bnd
Wald Upr Bnd Y1 -0.3997 -0.4 -0.4151 -0.3842

Likelihood ratio tests about transformation parameters LRT df pval LR
test, lambda = (0) 3334.454 1 0 LR test, lambda = (1) 48980.134 1 0

Call: lm(formula = I((TARGET\_AMT + 1)\^{}(-0.4)) \textasciitilde{} .,
data = dfCont)

Residuals: Min 1Q Median 3Q Max -1.22372 -0.26618 0.09885 0.27222
0.94841

Coefficients: Estimate Std. Error t value
Pr(\textgreater{}\textbar{}t\textbar{})\\
(Intercept) 5.718e-01 5.158e-02 11.087 \textless{} 2e-16 \textbf{\emph{
KIDSDRIV -4.597e-02 1.224e-02 -3.756 0.000175 }} AGE 7.686e-04 7.725e-04
0.995 0.319821\\
HOMEKIDS 3.092e-03 7.000e-03 0.442 0.658701\\
YOJ 1.362e-03 1.600e-03 0.851 0.394739\\
INCOME 3.641e-07 2.238e-07 1.627 0.103780\\
PARENT1Yes -1.008e-01 2.175e-02 -4.633 3.71e-06 \textbf{\emph{ HOME\_VAL
1.613e-07 6.999e-08 2.304 0.021244 }\\
MSTATUSz\_No -5.537e-02 1.647e-02 -3.362 0.000781 }\emph{ SEXz\_F
1.657e-02 1.982e-02 0.836 0.403263\\
EDUCATIONBachelors 5.935e-02 2.191e-02 2.709 0.006782 \textbf{
EDUCATIONMasters 5.585e-02 3.307e-02 1.689 0.091322 .\\
EDUCATIONPhD -1.270e-02 4.109e-02 -0.309 0.757357\\
EDUCATIONz\_High School -4.987e-03 1.811e-02 -0.275 0.783029\\
JOBDoctor 1.581e-01 4.858e-02 3.255 0.001140 } JOBHome Maker 4.226e-02
2.686e-02 1.573 0.115690\\
JOBLawyer 3.300e-02 3.293e-02 1.002 0.316351\\
JOBManager 1.629e-01 2.531e-02 6.438 1.34e-10 }\textbf{ JOBProfessional
5.427e-02 2.325e-02 2.334 0.019625 *\\
JOBStudent 4.755e-02 2.588e-02 1.837 0.066220 .\\
JOBz\_Blue Collar 4.227e-02 2.028e-02 2.084 0.037178 *\\
TRAVTIME -2.493e-03 3.520e-04 -7.080 1.66e-12 }\emph{ CAR\_USEPrivate
1.393e-01 1.785e-02 7.801 7.62e-15 }\textbf{ BLUEBOOK 2.603e-06
9.372e-07 2.778 0.005501 } TIF 7.735e-03 1.349e-03 5.734 1.05e-08
\textbf{\emph{ CAR\_TYPEPanel Truck -6.329e-02 3.233e-02 -1.957 0.050359
.\\
CAR\_TYPEPickup -7.585e-02 1.829e-02 -4.147 3.43e-05 }} CAR\_TYPESports
Car -1.518e-01 2.282e-02 -6.650 3.29e-11 \emph{\textbf{ CAR\_TYPEVan
-6.372e-02 2.397e-02 -2.659 0.007873 } CAR\_TYPEz\_SUV -9.798e-02
1.892e-02 -5.179 2.33e-07 }\textbf{ RED\_CARyes 2.835e-02 1.674e-02
1.694 0.090340 .\\
OLDCLAIM 2.323e-06 8.108e-07 2.865 0.004186 } CLM\_FREQ -3.064e-02
6.079e-03 -5.041 4.82e-07 \textbf{\emph{ REVOKEDYes -1.255e-01 1.939e-02
-6.471 1.08e-10 }} MVR\_PTS -2.202e-02 2.826e-03 -7.791 8.19e-15
\textbf{\emph{ CAR\_AGE 9.464e-04 1.415e-03 0.669 0.503748\\
URBANICITYz\_Highly Rural/ Rural 2.970e-01 1.493e-02 19.898 \textless{}
2e-16 }} --- Signif. codes: 0 `\emph{\textbf{' 0.001 '}' 0.01 '}' 0.05
`.' 0.1 `' 1

Residual standard error: 0.3699 on 4468 degrees of freedom Multiple
R-squared: 0.2444, Adjusted R-squared: 0.2383 F-statistic: 40.13 on 36
and 4468 DF, p-value: \textless{} 2.2e-16

\hypertarget{regular-logostic}{%
\subsection{Regular logostic}\label{regular-logostic}}

Call: glm(formula = TARGET\_FLAG \textasciitilde{} ., data = dfLog)

Deviance Residuals: Min 1Q Median 3Q Max\\
-0.9859 -0.2831 -0.1032 0.2783 1.2625

Coefficients: Estimate Std. Error t value
Pr(\textgreater{}\textbar{}t\textbar{})\\
(Intercept) 4.462e-01 5.365e-02 8.316 \textless{} 2e-16 \textbf{\emph{
KIDSDRIV 4.778e-02 1.273e-02 3.753 0.000177 }} AGE -8.077e-04 8.036e-04
-1.005 0.314936\\
HOMEKIDS -3.191e-03 7.282e-03 -0.438 0.661319\\
YOJ -1.397e-03 1.665e-03 -0.839 0.401432\\
INCOME -3.775e-07 2.328e-07 -1.622 0.104895\\
PARENT1Yes 1.043e-01 2.263e-02 4.607 4.19e-06 \textbf{\emph{ HOME\_VAL
-1.665e-07 7.281e-08 -2.286 0.022276 }\\
MSTATUSz\_No 5.775e-02 1.713e-02 3.370 0.000757 }\emph{ SEXz\_F
-1.718e-02 2.062e-02 -0.833 0.404608\\
EDUCATIONBachelors -6.133e-02 2.279e-02 -2.690 0.007162 \textbf{
EDUCATIONMasters -5.796e-02 3.440e-02 -1.685 0.092145 .\\
EDUCATIONPhD 1.286e-02 4.274e-02 0.301 0.763509\\
EDUCATIONz\_High School 5.205e-03 1.884e-02 0.276 0.782319\\
JOBDoctor -1.644e-01 5.053e-02 -3.253 0.001149 } JOBHome Maker
-4.396e-02 2.794e-02 -1.574 0.115658\\
JOBLawyer -3.464e-02 3.426e-02 -1.011 0.311951\\
JOBManager -1.696e-01 2.633e-02 -6.444 1.29e-10 }\textbf{
JOBProfessional -5.725e-02 2.419e-02 -2.367 0.017962 *\\
JOBStudent -4.933e-02 2.692e-02 -1.832 0.066990 .\\
JOBz\_Blue Collar -4.370e-02 2.110e-02 -2.071 0.038385 *\\
TRAVTIME 2.589e-03 3.662e-04 7.071 1.78e-12 }\emph{ CAR\_USEPrivate
-1.449e-01 1.857e-02 -7.802 7.52e-15 }\textbf{ BLUEBOOK -2.743e-06
9.749e-07 -2.814 0.004916 } TIF -8.044e-03 1.403e-03 -5.732 1.06e-08
\textbf{\emph{ CAR\_TYPEPanel Truck 6.574e-02 3.363e-02 1.955 0.050694
.\\
CAR\_TYPEPickup 7.887e-02 1.903e-02 4.146 3.45e-05 }} CAR\_TYPESports
Car 1.575e-01 2.374e-02 6.632 3.71e-11 \emph{\textbf{ CAR\_TYPEVan
6.581e-02 2.493e-02 2.640 0.008331 } CAR\_TYPEz\_SUV 1.015e-01 1.968e-02
5.159 2.59e-07 }\textbf{ RED\_CARyes -2.978e-02 1.741e-02 -1.710
0.087280 .\\
OLDCLAIM -2.447e-06 8.434e-07 -2.901 0.003740 } CLM\_FREQ 3.222e-02
6.324e-03 5.096 3.62e-07 \textbf{\emph{ REVOKEDYes 1.308e-01 2.017e-02
6.484 9.92e-11 }} MVR\_PTS 2.279e-02 2.940e-03 7.752 1.11e-14
\textbf{\emph{ CAR\_AGE -9.862e-04 1.472e-03 -0.670 0.503048\\
URBANICITYz\_Highly Rural/ Rural -3.087e-01 1.553e-02 -19.879
\textless{} 2e-16 }} --- Signif. codes: 0 `\emph{\textbf{' 0.001 '}'
0.01 '}' 0.05 `.' 0.1 `' 1

(Dispersion parameter for gaussian family taken to be 0.1480396)

\begin{verbatim}
Null deviance: 875.19  on 4504  degrees of freedom
\end{verbatim}

Residual deviance: 661.44 on 4468 degrees of freedom AIC: 4217.7

Number of Fisher Scoring iterations: 2

\hypertarget{lasso-logistic}{%
\subsection{LASSO Logistic}\label{lasso-logistic}}

\begin{verbatim}
## Loading required package: gplots
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'gplots'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:stats':
## 
##     lowess
\end{verbatim}

\begin{verbatim}
## Warning in aucDF$lasso.prob <- predict(lasso.model, type = "response", newx
## = X, : Coercing LHS to a list
\end{verbatim}

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-15-1.pdf}

\begin{verbatim}
## 24 x 1 sparse Matrix of class "dgCMatrix"
##                         1
## (Intercept)  9.311645e-01
## KIDSDRIV     2.104515e-01
## AGE         -3.066729e-03
## HOMEKIDS     .           
## YOJ          .           
## INCOME      -5.091546e-06
## PARENT1      5.400654e-01
## HOME_VAL    -1.319478e-06
## MSTATUS      2.495976e-01
## SEX          .           
## EDUCATION    1.950305e-02
## JOB         -3.135790e-04
## TRAVTIME     1.298654e-02
## CAR_USE     -8.073200e-01
## BLUEBOOK    -1.739624e-05
## TIF         -3.631229e-02
## CAR_TYPE     9.955364e-02
## RED_CAR     -3.785589e-02
## OLDCLAIM     .           
## CLM_FREQ     1.374393e-01
## REVOKED      4.762156e-01
## MVR_PTS      1.130536e-01
## CAR_AGE     -1.716225e-02
## URBANICITY  -1.888708e+00
\end{verbatim}

\hypertarget{theoretical-model}{%
\subsubsection{Theoretical Model}\label{theoretical-model}}

This model is selected for variables that I think will play a larger
role based on my prior belifes.

\hypertarget{chose-a-model}{%
\section{Chose a Model}\label{chose-a-model}}

Here we chose the best of each class of model.

\hypertarget{ols-models}{%
\subsection{OLS Models}\label{ols-models}}

\hypertarget{regular-ols}{%
\subsubsection{Regular OLS}\label{regular-ols}}

\begin{verbatim}
##      rstudent unadjusted p-value Bonferonni p
## 2988 19.17386         8.3009e-79   3.7395e-75
## 3916 17.71827         6.0975e-68   2.7469e-64
## 1393 16.20840         1.8631e-57   8.3935e-54
## 309  13.01407         4.9631e-38   2.2359e-34
## 2422 12.82457         5.3304e-37   2.4013e-33
## 3223 12.77978         9.2987e-37   4.1891e-33
## 2848 12.43057         6.6975e-35   3.0172e-31
## 586  11.88084         4.5067e-32   2.0303e-28
## 3419 11.54189         2.1799e-30   9.8206e-27
## 246  10.19349         3.8883e-24   1.7517e-20
\end{verbatim}

\begin{verbatim}
## Warning in rlm.default(x, y, weights, method = method, wt.method =
## wt.method, : 'rlm' failed to converge in 20 steps
\end{verbatim}

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-17-1.pdf}
\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-17-2.pdf}
\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-17-3.pdf}

\begin{verbatim}
## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 1937.732    Df = 1     p = 0
\end{verbatim}

\begin{verbatim}
## Warning in spreadLevelPlot.lm(fit): 631 negative fitted values removed
\end{verbatim}

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-17-4.pdf}
\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-17-5.pdf}

\begin{verbatim}
##  lag Autocorrelation D-W Statistic p-value
##    1    -0.003806213      2.007557   0.892
##  Alternative hypothesis: rho != 0
\end{verbatim}

We can see from the QQ Plot that we have some serious troubles with this
model.

Let's hope that we can find someting better. \#\#\# Log Scaled Model

\begin{verbatim}
## 
## No Studentized residuals with Bonferonni p < 0.05
## Largest |rstudent|:
##      rstudent unadjusted p-value Bonferonni p
## 2834 3.464935         0.00053535           NA
\end{verbatim}

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-18-1.pdf}
\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-18-2.pdf}
\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-18-3.pdf}

\begin{verbatim}
## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 419.1293    Df = 1     p = 3.77547e-93
\end{verbatim}

\begin{verbatim}
## Warning in spreadLevelPlot.lm(fit): 534 negative fitted values removed
\end{verbatim}

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-18-4.pdf}
\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-18-5.pdf}

\begin{verbatim}
##  lag Autocorrelation D-W Statistic p-value
##    1     -0.02836693      2.056647   0.038
##  Alternative hypothesis: rho != 0
\end{verbatim}

This model looks better but it is still far from perfect. The QQ Plot is
greatly improved but there are still issues with cooks distance etc.

\begin{verbatim}
## 
## No Studentized residuals with Bonferonni p < 0.05
## Largest |rstudent|:
##          rstudent unadjusted p-value Bonferonni p
## 2834 -3.32102....         0.00090407           NA
\end{verbatim}

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-19-1.pdf}
\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-19-2.pdf}
\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-19-3.pdf}
\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-19-4.pdf}

\begin{verbatim}
## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 394.5613    Df = 1     p = 8.411692e-88
\end{verbatim}

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-19-5.pdf}

\begin{verbatim}
##  lag Autocorrelation D-W Statistic p-value
##    1     -0.03132353      2.062565   0.018
##  Alternative hypothesis: rho != 0
\end{verbatim}

Even this model with a Box-Cox transformation has not great results.
Given these results on our transformed models, I think that it might be
worthwile examaning non-linear models such as tree based models. This,
however, goes beyond the scope of the course.

\hypertarget{logisic-models}{%
\subsection{Logisic Models}\label{logisic-models}}

\hypertarget{basic-logistic-regression}{%
\subsubsection{Basic Logistic
regression}\label{basic-logistic-regression}}

\begin{verbatim}
## Warning: package 'pROC' was built under R version 3.4.4
\end{verbatim}

\begin{verbatim}
## Type 'citation("pROC")' for a citation.
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'pROC'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:plotROC':
## 
##     ggroc
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:glmnet':
## 
##     auc
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     cov, smooth, var
\end{verbatim}

\begin{verbatim}
## [[1]]
\end{verbatim}

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-20-1.pdf}

\begin{verbatim}
## 
## [[2]]
## Area under the curve: 0.8207
\end{verbatim}

\hypertarget{section}{%
\subsubsection{}\label{section}}

The theoretical model performs much worse than the model with everyting
in it.

\hypertarget{lasso-logsitic}{%
\subsubsection{LASSO Logsitic}\label{lasso-logsitic}}

\includegraphics{KLukowiakA4_files/figure-latex/unnamed-chunk-22-1.pdf}

\begin{verbatim}
## [[1]]
## [1] 0.8087627
\end{verbatim}

\hypertarget{model-selection}{%
\section{Model Selection}\label{model-selection}}

\hypertarget{ols}{%
\subsection{OLS}\label{ols}}

I will base my selection off of the \(R^2\) value. As such, we select
the log transfomred logistic regression.

\hypertarget{glm-selection.}{%
\subsection{GLM Selection.}\label{glm-selection.}}

For the GLM Model we will use AUC as a selector. As such, I chose the
baisc model.

\hypertarget{predictions}{%
\section{Predictions}\label{predictions}}

\hypertarget{ols-1}{%
\subsection{OLS}\label{ols-1}}

\begin{verbatim}
##          1          2          3          4          5          6 
## 1.07923964 2.50448156 1.98003329 1.54566376 3.63463698 0.05468482
\end{verbatim}

\hypertarget{glm}{%
\subsection{GLM}\label{glm}}

\begin{verbatim}
##          1          2          3          4          5          6 
## 0.12471795 0.29952584 0.24145782 0.17842149 0.43940253 0.01132441
\end{verbatim}

\hypertarget{apendix}{%
\section{Apendix}\label{apendix}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\OperatorTok{::}\NormalTok{opts_chunk}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{echo =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{knitr}\OperatorTok{::}\NormalTok{opts_knit}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{root.dir =} \StringTok{"/Users/kailukowiak/DATA621/Assignments"}\NormalTok{)}
\NormalTok{knitr}\OperatorTok{::}\NormalTok{opts_chunk}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{echo=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{# Libraries}
\NormalTok{##################}
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{library}\NormalTok{(car)}
\KeywordTok{library}\NormalTok{(leaps)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{library}\NormalTok{(kableExtra)}
\KeywordTok{library}\NormalTok{(psych)}
\KeywordTok{library}\NormalTok{(ggthemes)}
\KeywordTok{library}\NormalTok{(corrplot)}
\KeywordTok{library}\NormalTok{(glmnet)}
\KeywordTok{library}\NormalTok{(bestglm)}
\KeywordTok{library}\NormalTok{(xtable)}
\KeywordTok{library}\NormalTok{(caTools)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{xtable.floating =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{xtable.timestamp =} \StringTok{""}\NormalTok{)}
\NormalTok{####################}
\NormalTok{moneyCV <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(df)\{}
  \ControlFlowTok{for}\NormalTok{ (colName }\ControlFlowTok{in} \KeywordTok{names}\NormalTok{(df))\{}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{grepl}\NormalTok{(}\StringTok{'}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{$'}\NormalTok{, df[, colName]))\{}
\NormalTok{      df[, colName] =}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{$|,"}\NormalTok{, }\StringTok{""}\NormalTok{, df[[colName]]) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{()}
\NormalTok{    \}}
\NormalTok{  \}}
  \KeywordTok{return}\NormalTok{(df)}
\NormalTok{\}}

\NormalTok{factorCV <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(df)\{}
  \ControlFlowTok{for}\NormalTok{ (colName }\ControlFlowTok{in} \KeywordTok{names}\NormalTok{(df))\{}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{is.character}\NormalTok{(df[[colName]])) \{}
\NormalTok{      df[, colName] =}\StringTok{ }\NormalTok{df[[colName]] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.factor}\NormalTok{()}
\NormalTok{    \}}
\NormalTok{  \}}
  \KeywordTok{return}\NormalTok{(df)}
\NormalTok{\}}
\CommentTok{# Loading the data}

\NormalTok{LabledDF <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{'Assignment4/insurance_training_data.csv'}\NormalTok{)}

\NormalTok{LabledDF <-}\StringTok{ }\KeywordTok{moneyCV}\NormalTok{(LabledDF)}
\NormalTok{LabledDF <-}\StringTok{ }\KeywordTok{factorCV}\NormalTok{(LabledDF)}
\NormalTok{LabledDF <-}\StringTok{ }\NormalTok{LabledDF }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{INDEX)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{) }
\NormalTok{sample =}\StringTok{ }\KeywordTok{sample.split}\NormalTok{(LabledDF}\OperatorTok{$}\NormalTok{TARGET_FLAG, }\DataTypeTok{SplitRatio =} \FloatTok{.75}\NormalTok{)}
\NormalTok{df <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(LabledDF, sample }\OperatorTok{==}\StringTok{ }\OtherTok{TRUE}\NormalTok{)}
\NormalTok{testDF <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(LabledDF, sample }\OperatorTok{==}\StringTok{ }\OtherTok{FALSE}\NormalTok{)}
\NormalTok{temp <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{sample_n}\NormalTok{(}\DecValTok{6}\NormalTok{)}
\NormalTok{temp }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{()}
\CommentTok{# temp[,1:8] %>% kable(caption = 'Sample of Values for the Training Set')}
\CommentTok{# temp[9:17] %>% kable(caption = 'Sample of Values for the Training Set')}
\CommentTok{# temp[18:25] %>% kable(caption = 'Sample of Values for the Training Set')}
\KeywordTok{glimpse}\NormalTok{(df) }
\NormalTok{evalDF <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{'/Users/kailukowiak/DATA621/Assignments/Assignment4/insurance-evaluation-data.csv'}\NormalTok{)}
\NormalTok{evalDF <-}\StringTok{ }\KeywordTok{moneyCV}\NormalTok{(evalDF)}
\NormalTok{evalDF <-}\StringTok{ }\KeywordTok{factorCV}\NormalTok{(evalDF)}
\NormalTok{evalDF <-}\StringTok{ }\NormalTok{evalDF }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{INDEX, }\OperatorTok{-}\NormalTok{TARGET_AMT, }\OperatorTok{-}\NormalTok{TARGET_FLAG)}
\NormalTok{temp <-}\StringTok{ }\NormalTok{evalDF }\OperatorTok{%>%}\StringTok{ }\KeywordTok{sample_n}\NormalTok{(}\DecValTok{6}\NormalTok{)}
\NormalTok{temp[}\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{'Sample of Values for the Test Set'}\NormalTok{)}
\NormalTok{###############################}
\KeywordTok{setwd}\NormalTok{(}\StringTok{"~/DATA621/Assignments"}\NormalTok{)}
\NormalTok{lables <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{'Assignment4/dataLegend.csv'}\NormalTok{)}
\NormalTok{lables }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{()}
\CommentTok{# Summary Tables}
\NormalTok{SumTab <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(df)}
\NormalTok{SumTab1 <-}\StringTok{ }\NormalTok{SumTab[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{]}
\NormalTok{SumTab2 <-}\StringTok{ }\NormalTok{SumTab[, }\DecValTok{7}\OperatorTok{:}\DecValTok{13}\NormalTok{]}
\KeywordTok{kable}\NormalTok{(SumTab1, }\DataTypeTok{caption =} \StringTok{'Summary Statistics'}\NormalTok{)}
\KeywordTok{kable}\NormalTok{(SumTab2, }\DataTypeTok{caption =} \StringTok{'Summary Statistics'}\NormalTok{)}
\NormalTok{#####################}
\NormalTok{dis <-}\StringTok{ }\KeywordTok{describe}\NormalTok{(df)}
\NormalTok{dis[, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{'Descriptive Statistics'}\NormalTok{)}
\NormalTok{dis[, }\DecValTok{6}\OperatorTok{:}\DecValTok{9}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{'Descriptive Statistics'}\NormalTok{)}
\NormalTok{dis[, }\DecValTok{10}\OperatorTok{:}\DecValTok{13}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{'Descriptive Statistics'}\NormalTok{)}
\NormalTok{temp <-}\StringTok{ }\KeywordTok{map}\NormalTok{(df, }\OperatorTok{~}\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(.))) }
\NormalTok{temp <-}\StringTok{ }\KeywordTok{t}\NormalTok{(temp)}
\NormalTok{temp[}\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{'Count of NA Values'}\NormalTok{)}
\NormalTok{temp[}\DecValTok{9}\OperatorTok{:}\DecValTok{17}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{'Count of NA Values'}\NormalTok{)}
\NormalTok{temp[}\DecValTok{18}\OperatorTok{:}\DecValTok{25}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{'Count of NA Values'}\NormalTok{)}

\NormalTok{df}\OperatorTok{$}\NormalTok{CONTAINS_NA <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{complete.cases}\NormalTok{(df), }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{)}

\NormalTok{corFlag <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(df}\OperatorTok{$}\NormalTok{TARGET_FLAG, df}\OperatorTok{$}\NormalTok{CONTAINS_NA)}
\NormalTok{corAmt <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(df}\OperatorTok{$}\NormalTok{TARGET_AMT, df}\OperatorTok{$}\NormalTok{CONTAINS_NA)}
\NormalTok{df <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{CONTAINS_NA)}
\NormalTok{df <-}\StringTok{ }\NormalTok{df[}\KeywordTok{complete.cases}\NormalTok{(df),]}
\NormalTok{evalDF <-}\StringTok{ }\NormalTok{evalDF[}\KeywordTok{complete.cases}\NormalTok{(evalDF),]}
\NormalTok{testDF <-}\StringTok{ }\NormalTok{testDF[}\KeywordTok{complete.cases}\NormalTok{(testDF),]}
\NormalTok{df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select_if}\NormalTok{(is.numeric) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{scale}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ key, }\DataTypeTok{y =}\NormalTok{ value)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_tufte}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\CommentTok{#geom_tufteboxplot(outlier.colour="black")+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.title=}\KeywordTok{element_blank}\NormalTok{()) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{'Scaled Values'}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{'Variable'}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{'Distrobution of Values'}\NormalTok{, }\DataTypeTok{subtitle =} \StringTok{'Y values scaled to fit a common axis'}\NormalTok{)}
\NormalTok{df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select_if}\NormalTok{(is.numeric) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{scale}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ key, }\DataTypeTok{y =}\NormalTok{ value)) }\OperatorTok{+}
\StringTok{  }\CommentTok{# geom_violin()+}
\StringTok{  }\CommentTok{# geom_tufteboxplot(outlier.colour="black", outlier.shape = 22)+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_tufte}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.title=}\KeywordTok{element_blank}\NormalTok{()) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{'Scaled Values'}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{'Variable'}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{'Distrobution of Values'}\NormalTok{, }\DataTypeTok{subtitle =} \StringTok{'Y values scaled to fit a common axis'}\NormalTok{)}
\NormalTok{df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select_if}\NormalTok{(is.factor) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{value))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{key,}\DataTypeTok{scales=}\StringTok{'free_x'}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{))}
\NormalTok{dfCont <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{TARGET_FLAG)}
\NormalTok{dfLog <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{TARGET_AMT)}
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(TARGET_AMT }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dfCont)}
\CommentTok{#summary(mod1)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{xtable.comment =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(mod1)}
\NormalTok{mod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(TARGET_AMT}\OperatorTok{+}\DecValTok{1}\NormalTok{) }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dfCont) }\CommentTok{# Note the `+1`}
\KeywordTok{summary}\NormalTok{(mod2)}

\CommentTok{# Box Cox Method, univariate}
\KeywordTok{summary}\NormalTok{(p1 <-}\StringTok{ }\KeywordTok{powerTransform}\NormalTok{(}\KeywordTok{I}\NormalTok{(TARGET_AMT}\OperatorTok{+}\DecValTok{1}\NormalTok{) }\OperatorTok{~}\StringTok{ }\NormalTok{., dfCont))}

\NormalTok{bcTrans <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{I}\NormalTok{((TARGET_AMT}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\FloatTok{0.4}\NormalTok{)) }\OperatorTok{~}\StringTok{ }\NormalTok{., dfCont)}
\KeywordTok{summary}\NormalTok{(bcTrans) }
\NormalTok{logMod1 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(TARGET_FLAG}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dfLog)}
\NormalTok{logMod1 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summary}\NormalTok{()}
\NormalTok{X <-}\StringTok{ }\NormalTok{dfLog }\OperatorTok{%>%}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{TARGET_FLAG)}
\NormalTok{X <-}\StringTok{ }\KeywordTok{data.matrix}\NormalTok{(X)}
\CommentTok{#X <- as.matrix(X, ncol=12)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(dfLog}\OperatorTok{$}\NormalTok{TARGET_FLAG)}
\NormalTok{fit =}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(X, y, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(ROCR)}
\NormalTok{aucDF <-}\StringTok{ }\NormalTok{X}
\NormalTok{lasso.model =}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(X, y, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{type.measure =} \StringTok{'class'}\NormalTok{)}

\NormalTok{aucDF}\OperatorTok{$}\NormalTok{lasso.prob <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lasso.model, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{, }\DataTypeTok{newx =}\NormalTok{ X, }\DataTypeTok{s =} \StringTok{'lambda.1se'}\NormalTok{)}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{prediction}\NormalTok{(aucDF}\OperatorTok{$}\NormalTok{lasso.prob, y)}


\NormalTok{cvfit =}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(X, y, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{type.measure =} \StringTok{"class"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(cvfit)}
\KeywordTok{coef}\NormalTok{(cvfit, }\DataTypeTok{s =} \StringTok{"lambda.1se"}\NormalTok{)}
\CommentTok{# theoDF <- dfLog %>% select(TARGET_FLAG, KIDSDRIV, AGE, OLDCLAIM, REVOKED, RED_CAR,)}
\CommentTok{# modTheo <- glm(TARGET_FLAG~., data = theoDF)}
\CommentTok{# summary(modTheo) %>% xtable()}
\NormalTok{regressionDiagnostic <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(fit)\{}
\NormalTok{  ## https://www.statmethods.net/stats/rdiagnostics.html}
  \KeywordTok{library}\NormalTok{(car) }\CommentTok{# Required}
  \KeywordTok{print}\NormalTok{(}\KeywordTok{outlierTest}\NormalTok{(fit))}
  \KeywordTok{qqPlot}\NormalTok{(fit, }\DataTypeTok{main =} \StringTok{'QQ Plot'}\NormalTok{)}
  \CommentTok{#av.Plots(fit)}
\NormalTok{  cutoff <-}\StringTok{ }\DecValTok{4}\OperatorTok{/}\NormalTok{((}\KeywordTok{nrow}\NormalTok{(dfCont)}\OperatorTok{-}\KeywordTok{length}\NormalTok{(fit}\OperatorTok{$}\NormalTok{coefficients)}\OperatorTok{-}\DecValTok{2}\NormalTok{)) }
  \KeywordTok{plot}\NormalTok{(fit, }\DataTypeTok{which=}\DecValTok{4}\NormalTok{, }\DataTypeTok{cook.levels=}\NormalTok{cutoff)}
  \CommentTok{# Influence Plot }
  \KeywordTok{influencePlot}\NormalTok{(fit,    }\DataTypeTok{id.method=}\StringTok{"identify"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Influence Plot"}\NormalTok{, }
                \DataTypeTok{sub=}\StringTok{"Circle size is proportial to Cook's Distance"}\NormalTok{ )}
  \KeywordTok{library}\NormalTok{(MASS)}
\NormalTok{  sresid <-}\StringTok{ }\KeywordTok{studres}\NormalTok{(fit) }
  \KeywordTok{hist}\NormalTok{(sresid, }\DataTypeTok{freq=}\OtherTok{FALSE}\NormalTok{, }
       \DataTypeTok{main=}\StringTok{"Distribution of Studentized Residuals"}\NormalTok{)}
\NormalTok{  xfit<-}\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(sresid),}\KeywordTok{max}\NormalTok{(sresid),}\DataTypeTok{length=}\DecValTok{40}\NormalTok{) }
\NormalTok{  yfit<-}\KeywordTok{dnorm}\NormalTok{(xfit) }
  \KeywordTok{lines}\NormalTok{(xfit, yfit)}
  \KeywordTok{print}\NormalTok{(}\KeywordTok{ncvTest}\NormalTok{(fit))}
  \CommentTok{# plot studentized residuals vs. fitted values }
  \KeywordTok{spreadLevelPlot}\NormalTok{(fit)}
  \KeywordTok{print}\NormalTok{(}\KeywordTok{durbinWatsonTest}\NormalTok{(fit))}
\NormalTok{\}}
\KeywordTok{regressionDiagnostic}\NormalTok{(mod1)}
\KeywordTok{regressionDiagnostic}\NormalTok{(mod2)}
\KeywordTok{regressionDiagnostic}\NormalTok{(bcTrans)}
\NormalTok{AUC <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(df, mod, modelName)\{}
  \KeywordTok{library}\NormalTok{(plotROC)}
  \KeywordTok{library}\NormalTok{(pROC)}
\NormalTok{  prob =}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod,}\DataTypeTok{type =} \KeywordTok{c}\NormalTok{(}\StringTok{"response"}\NormalTok{))}
\NormalTok{  dfLog}\OperatorTok{$}\NormalTok{prob=prob}
\NormalTok{  p =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dfLog, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{d =}\NormalTok{ TARGET_FLAG, }\DataTypeTok{m =}\NormalTok{ prob)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_roc}\NormalTok{(}\DataTypeTok{n.cuts =} \DecValTok{0}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{'AUC Graph for'}\NormalTok{, modelName)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{xlab}\NormalTok{(}\StringTok{"False Positive Fraction"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{'True Positive Fraction'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{linetype =} \StringTok{'dashed'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_tufte}\NormalTok{()}
  
\NormalTok{  g <-}\StringTok{ }\KeywordTok{roc}\NormalTok{(TARGET_FLAG }\OperatorTok{~}\StringTok{ }\NormalTok{prob, }\DataTypeTok{data =}\NormalTok{ dfLog)}
  \KeywordTok{return}\NormalTok{(}\KeywordTok{list}\NormalTok{(p, g}\OperatorTok{$}\NormalTok{auc))}
\NormalTok{\}}
\KeywordTok{AUC}\NormalTok{(}\KeywordTok{select}\NormalTok{(testDF, }\OperatorTok{-}\NormalTok{TARGET_AMT), logMod1, }\StringTok{'Baisic GLM Model'}\NormalTok{)}

\CommentTok{#AUC(select(testDF, -TARGET_AMT), modTheo, 'Theoretical Model')}
\CommentTok{#testX <- a}
\NormalTok{fittedGLMcv <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(cvfit, X, }\DataTypeTok{s =} \StringTok{"lambda.1se"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}

\NormalTok{perf <-}\StringTok{ }\KeywordTok{performance}\NormalTok{(pred,}\StringTok{"tpr"}\NormalTok{,}\StringTok{"fpr"}\NormalTok{)}
\NormalTok{auc <-}\StringTok{  }\KeywordTok{performance}\NormalTok{(pred,}\StringTok{"auc"}\NormalTok{) }\CommentTok{# shows calculated AUC for model}
\NormalTok{auc <-}\StringTok{ }\NormalTok{auc}\OperatorTok{@}\NormalTok{y.values}

\NormalTok{roc.data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{fpr=}\KeywordTok{unlist}\NormalTok{(perf}\OperatorTok{@}\NormalTok{x.values),}
                       \DataTypeTok{tpr=}\KeywordTok{unlist}\NormalTok{(perf}\OperatorTok{@}\NormalTok{y.values))}

\KeywordTok{ggplot}\NormalTok{(roc.data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{fpr, }\DataTypeTok{ymin=}\DecValTok{0}\NormalTok{, }\DataTypeTok{ymax=}\NormalTok{tpr)) }\OperatorTok{+}
\StringTok{    }\CommentTok{#geom_ribbon(alpha=0.2) +}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{tpr)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{slope=}\DecValTok{1}\NormalTok{, }\DataTypeTok{intercept=}\DecValTok{0}\NormalTok{, }\DataTypeTok{linetype=}\StringTok{'dashed'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"ROC Curve"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{'True Positive Rate'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{xlab}\NormalTok{(}\StringTok{'False Positive Rate'}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_tufte}\NormalTok{()}
\NormalTok{auc}
\CommentTok{#evalDF <- evalDF %>% select(-CONTAINS_NA)}
\KeywordTok{predict}\NormalTok{(mod2,}\DataTypeTok{newdata =}\NormalTok{ evalDF) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\KeywordTok{predict}\NormalTok{(logMod1, }\DataTypeTok{newdata =}\NormalTok{ evalDF) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}


\end{document}
