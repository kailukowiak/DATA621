---
title: "Assignment 4"
author: "Kai Lukowiak"
date: '2018-03-25'
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
abstract: "This paper looks into the predictive ability of certain factors into the likelyhood of a person getting into an accident and also the amount that the accident will cost."
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir = "/Users/kailukowiak/DATA621/Assignments")
knitr::opts_chunk$set(echo=FALSE)
```


```{r libraries, message=FALSE, warning=FALSE, echo=FALSE}
# Libraries
##################
library(MASS)
library(car)
library(leaps)
library(tidyverse)
library(knitr)
library(kableExtra)
library(psych)
library(ggthemes)
library(corrplot)
library(glmnet)
library(bestglm)
library(xtable)
library(caTools)
options(xtable.floating = FALSE)
options(xtable.timestamp = "")
####################
```

```{r cleaningFunctions}
moneyCV <- function(df){
  for (colName in names(df)){
    if (grepl('\\$', df[, colName])){
      df[, colName] = gsub("\\$|,", "", df[[colName]]) %>% as.numeric()
    }
  }
  return(df)
}

factorCV <- function(df){
  for (colName in names(df)){
    if (is.character(df[[colName]])) {
      df[, colName] = df[[colName]] %>% as.factor()
    }
  }
  return(df)
}
```


# Data Exploration

## The Data Frames


```{r LoadData, warning=F, message=F, echo=FALSE}
# Loading the data

LabledDF <- read_csv('Assignment4/insurance_training_data.csv')

LabledDF <- moneyCV(LabledDF)
LabledDF <- factorCV(LabledDF)
LabledDF <- LabledDF %>% select(-INDEX)
set.seed(101) 
sample = sample.split(LabledDF$TARGET_FLAG, SplitRatio = .75)
df <- subset(LabledDF, sample == TRUE)
testDF <- subset(LabledDF, sample == FALSE)
temp <- df %>% sample_n(6)
temp %>% kable()
# temp[,1:8] %>% kable(caption = 'Sample of Values for the Training Set')
# temp[9:17] %>% kable(caption = 'Sample of Values for the Training Set')
# temp[18:25] %>% kable(caption = 'Sample of Values for the Training Set')
```

```{r}
glimpse(df) 
```


The trainind dataset is comprised of 26 variables, two of which are response variables, `TARGET_FLAG` and `TARGET_AMT`. These will be used to run logistic and regular regression respectivly. 

The evaluation set looks similar but has `NA`s in the first two rows.

```{r LoadData2, warning=F, message=F, echo=FALSE}
evalDF <- read_csv('/Users/kailukowiak/DATA621/Assignments/Assignment4/insurance-evaluation-data.csv')
evalDF <- moneyCV(evalDF)
evalDF <- factorCV(evalDF)
evalDF <- evalDF %>% select(-INDEX, -TARGET_AMT, -TARGET_FLAG)
temp <- evalDF %>% sample_n(6)
temp[1:8] %>% kable(caption = 'Sample of Values for the Test Set')
###############################
```



The evaluation set is a similar data frame but excludes the target variable. As such it cannot be used for cross validation.

```{r}
setwd("~/DATA621/Assignments")
lables <- read_csv('Assignment4/dataLegend.csv')
lables %>% kable()
```


## Summary Statistics


```{r, echo=FALSE}
# Summary Tables
SumTab <- summary(df)
SumTab1 <- SumTab[, 1:6]
SumTab2 <- SumTab[, 7:13]
kable(SumTab1, caption = 'Summary Statistics')
kable(SumTab2, caption = 'Summary Statistics')
#####################
```


These tables give an overview of the variables, suggesting there may be some issues with distributions but we will need to look further before making any decisions on transforming the variables.

## Descriptive Statistics

```{r, echo=FALSE}
dis <- describe(df)
dis[, 1:5] %>% kable(caption = 'Descriptive Statistics')
dis[, 6:9] %>% kable(caption = 'Descriptive Statistics')
dis[, 10:13] %>% kable(caption = 'Descriptive Statistics')
```


The count of NA values for each variable is given below.

```{r echo=FALSE}
temp <- map(df, ~sum(is.na(.))) 
temp <- t(temp)
temp[1:8] %>% kable(caption = 'Count of NA Values')
temp[9:17] %>% kable(caption = 'Count of NA Values')
temp[18:25] %>% kable(caption = 'Count of NA Values')

```

There are quite a few missing values accross several variables. However, compared to the size of the training set, around 6000, these numbers could be dropped if there is no correlation between the missing values and the response variables.

```{r}
df$CONTAINS_NA <- ifelse(complete.cases(df), FALSE, TRUE)

corFlag <- cor(df$TARGET_FLAG, df$CONTAINS_NA)
corAmt <- cor(df$TARGET_AMT, df$CONTAINS_NA)
df <- df %>% select(-CONTAINS_NA)
```

The correlation between missing values and the 'Claim Filed' response is `r corFlag` and `r corAmt` for the claim amount. Since these are very close to zero we are not worried about them effecting the regressions. As such, we will drop them.

```{r}
df <- df[complete.cases(df),]
evalDF <- evalDF[complete.cases(evalDF),]
testDF <- testDF[complete.cases(testDF),]
```


## Graphical EDA

```{r, echo=FALSE}
df %>% 
  select_if(is.numeric) %>% 
  scale() %>% 
  as_tibble() %>% 
  gather() %>% 
  ggplot(aes(x = key, y = value)) +
  theme_tufte() +
  geom_violin()+
  #geom_tufteboxplot(outlier.colour="black")+
  theme(axis.title=element_blank()) +
  ylab('Scaled Values')+
  xlab('Variable')+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ggtitle('Distrobution of Values', subtitle = 'Y values scaled to fit a common axis')
```

The distrobutions are generally skeded upwards, nothing suggests problems with the dataset. The only variable that is very skewed is `TARGET_AMT` and this makes sense because most are zero or low and some are very high.

```{r, echo=FALSE}
df %>% 
  select_if(is.numeric) %>% 
  scale() %>% 
  as_tibble() %>% 
  gather() %>% 
  ggplot(aes(x = key, y = value)) +
  # geom_violin()+
  # geom_tufteboxplot(outlier.colour="black", outlier.shape = 22)+
  geom_boxplot()+
  theme_tufte() +
  theme(axis.title=element_blank()) +
  ylab('Scaled Values')+
  xlab('Variable')+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ggtitle('Distrobution of Values', subtitle = 'Y values scaled to fit a common axis')
```


From these two graphs we can see that many of distributions are skewed in one direction or another. It is also interesting to see that the target variable is below zero. This means that the median and mean values are different. 




```{r}
df %>% 
  select_if(is.factor) %>% 
  gather() %>% 
  ggplot(aes(x=value))+
  geom_bar()+
  facet_wrap(~key,scales='free_x')+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


The factor variables some uneven counts as well but nothing that is highly out of the ordinary.

# Data Preperation



## Transformed Skewed Variables

I will log transform `TARGET_AMT` in one of the models that build to account for the wide range. During this transformation it is important to add 1 to each variable because there are many zero values that would throw and error. 

I also transformed  `TARGET_AMT` with the power of -0.4 based off of a Box-Cox analysis. Given the nature of other variables it does not seem necessary to transform others.

# Build Models

Model selection will be based off of automated selection techniques as well as specific transformations.

## Ordinary Least Squares

```{r dataSplit}
dfCont <- df %>% select(-TARGET_FLAG)
dfLog <- df %>% select(-TARGET_AMT)
```

### Baisic Regression
Here I regressed all variables without transformation.

```{r results='asis'}
mod1 <- lm(TARGET_AMT ~ ., data = dfCont)
#summary(mod1)
options(xtable.comment = FALSE)
summary(mod1)
```

This is a pretty poor $R^2$. While there are some significant variables, the overall performance is poor.

### Log Transformed
Next we look at the regression with a log transformed `TARGET_AMT` variable.

```{r results='asis'}
mod2 <- lm(log(TARGET_AMT+1) ~ ., data = dfCont) # Note the `+1`
summary(mod2)
```

This is an improvement. It makes sense that we would need to transform the response variable given it's skewed nature.

## BoxCox
```{r results='asis'}

# Box Cox Method, univariate
summary(p1 <- powerTransform(I(TARGET_AMT+1) ~ ., dfCont))

bcTrans <- lm(I((TARGET_AMT+1)^(-0.4)) ~ ., dfCont)
summary(bcTrans) 
```


## Regular logostic


```{r results='asis'}
logMod1 <- glm(TARGET_FLAG~., data = dfLog)
logMod1 %>% summary()
```


## LASSO Logistic

```{r}
X <- dfLog %>% dplyr::select(-TARGET_FLAG)
X <- data.matrix(X)
#X <- as.matrix(X, ncol=12)
y <- as.factor(dfLog$TARGET_FLAG)
fit = glmnet(X, y, family = "binomial")
library(ROCR)
aucDF <- X
lasso.model = cv.glmnet(X, y, family = "binomial", type.measure = 'class')

aucDF$lasso.prob <- predict(lasso.model, type="response", newx = X, s = 'lambda.1se')
pred <- prediction(aucDF$lasso.prob, y)


cvfit = cv.glmnet(X, y, family = "binomial", type.measure = "class")
plot(cvfit)
coef(cvfit, s = "lambda.1se")
```

### Theoretical Model
This model is selected for variables that I think will play a larger role based on my prior belifes. 

```{r results='asis'}
# theoDF <- dfLog %>% select(TARGET_FLAG, KIDSDRIV, AGE, OLDCLAIM, REVOKED, RED_CAR,)
# modTheo <- glm(TARGET_FLAG~., data = theoDF)
# summary(modTheo) %>% xtable()
```



# Chose a Model
Here we chose the best of each class of model.

## OLS Models

### Regular OLS

```{r}
regressionDiagnostic <- function(fit){
  ## https://www.statmethods.net/stats/rdiagnostics.html
  library(car) # Required
  print(outlierTest(fit))
  qqPlot(fit, main = 'QQ Plot')
  #av.Plots(fit)
  cutoff <- 4/((nrow(dfCont)-length(fit$coefficients)-2)) 
  plot(fit, which=4, cook.levels=cutoff)
  # Influence Plot 
  influencePlot(fit,	id.method="identify", main="Influence Plot", 
                sub="Circle size is proportial to Cook's Distance" )
  library(MASS)
  sresid <- studres(fit) 
  hist(sresid, freq=FALSE, 
       main="Distribution of Studentized Residuals")
  xfit<-seq(min(sresid),max(sresid),length=40) 
  yfit<-dnorm(xfit) 
  lines(xfit, yfit)
  print(ncvTest(fit))
  # plot studentized residuals vs. fitted values 
  spreadLevelPlot(fit)
  print(durbinWatsonTest(fit))
}
regressionDiagnostic(mod1)
```

We can see from the QQ Plot that we have some serious troubles with this model. 

Let's hope that we can find someting better.
### Log Scaled Model


```{r}
regressionDiagnostic(mod2)
```

This model looks better but it is still far from perfect. The QQ Plot is greatly improved but there are still issues with cooks distance etc.


```{r}
regressionDiagnostic(bcTrans)
```


Even this model with a Box-Cox transformation has not great results. Given these results on our transformed models, I think that it might be worthwile examaning non-linear models such as tree based models. This, however, goes beyond the scope of the course.

## Logisic Models

### Basic Logistic regression
```{r}
AUC <- function(df, mod, modelName){
  library(plotROC)
  library(pROC)
  prob = predict(mod,type = c("response"))
  dfLog$prob=prob
  p = ggplot(dfLog, aes(d = TARGET_FLAG, m = prob)) + 
    geom_roc(n.cuts = 0) + 
    ggtitle(paste('AUC Graph for', modelName)) +
    xlab("False Positive Fraction") +
    ylab('True Positive Fraction') +
    geom_abline(linetype = 'dashed') +
    theme_tufte()
  
  g <- roc(TARGET_FLAG ~ prob, data = dfLog)
  return(list(p, g$auc))
}
AUC(select(testDF, -TARGET_AMT), logMod1, 'Baisic GLM Model')
```

### 

```{r}

#AUC(select(testDF, -TARGET_AMT), modTheo, 'Theoretical Model')
```

The theoretical model performs much worse than the model with everyting in it. 

### LASSO Logsitic 

```{r}
#testX <- a
fittedGLMcv <- predict(cvfit, X, s = "lambda.1se", type = "class")

perf <- performance(pred,"tpr","fpr")
auc <-  performance(pred,"auc") # shows calculated AUC for model
auc <- auc@y.values

roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values))

ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
    #geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    geom_abline(slope=1, intercept=0, linetype='dashed') +
    ggtitle("ROC Curve") +
    ylab('True Positive Rate') +
    xlab('False Positive Rate') + theme_tufte()
auc
```

# Model Selection

## OLS
I will base my selection off of the $R^2$ value. As such, we select the log transfomred logistic regression.

## GLM Selection.
For the GLM Model we will use AUC as a selector. As such, I chose the baisc model.

# Predictions

## OLS

```{r}
#evalDF <- evalDF %>% select(-CONTAINS_NA)
predict(mod2,newdata = evalDF) %>% head()
```


## GLM

```{r}
predict(logMod1, newdata = evalDF) %>% head()
```

# Apendix

```{r appendix, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
``` 

