---
title: "DATA 621 Assignment 2"
author: "Kai Lukowiak"
date: '2018-03-14'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE) # Necessary to compile to pdf
# Source: https://chrisbeeley.net/?p=1037
```

# Import Data


```{r warning=F}
library(tidyverse)
library(knitr)
df <- read_csv('~/DATA621/Assignments/Assignment2/classification-output-data.csv')
sample_n(df, size = 5) %>% kable()
```


# Confusion Matrix
R's table function can be used to create a confusion matrix. For an more indepth explenation of this please see [this](http://www.saedsayad.com/model_evaluation_c.htm) excelent website. 

```{r}
x <- table(df$class, df$scored.class)
colnames(x) <- c('Actual Negative ', 'Actual Positive')
rownames(x) <- c("Predicted Negative", 'Predicted Positive')
x %>% kable()
```

The sum of the rows and columns can give insight into model performance. The rows represent the predicted values while the columns represent the actual values. 



# Accuracy 
Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions.

```{r}
confusionFunction <- function(df, actual, predicted, metric){
  x <- table(df[[actual]], df[[predicted]])
  TN <- x[2, 2]; FN <- x[1, 2]; FP <- x[2, 1]; TP <- x[2, 2]
  # Values.
  
  if (metric == 'Accuracy'){
    Accuracy <- (TP + TN) / (TN + FN + FP + TP)
    return(Accuracy)
    }
  
  else if (metric == 'ClassificationErrorRate'){
    ClassificationErrorRate <- (FP + FN) / (TN + FN + FP + TP)
    return(ClassificationErrorRate)
  } 
  
  else if (metric == 'Precicion'){
     Precicion <- TP / (TP + FP)
     return( Precicion)
  }
  
  else if (metric == "Sensitivity"){
    Sensitivity <- TP / (TP + FN)
    return(Sensitivity)
  } 
  
  else {
    Specificity <- TN / (TN + FP)  
    return(Specificity)
  }
}

confusionFunction(df, 9, 10, "Accuracy")
```

# Classification Error Rate

```{r}
confusionFunction(df, 9, 10, 'ClassificationErrorRate')
```

To verify that these sum to one:

```{r}
confusionFunction(df, 9, 10, 'ClassificationErrorRate') + 
  confusionFunction(df, 9, 10, 'Accuracy')
```

This test is passed.

# Sensitivity

```{r}
confusionFunction(df, 9, 10, 'Sensitivity')
```

# Precision

```{r}
confusionFunction(df, 9, 10, 'Precicion')
```

# Specificity

```{r}
confusionFunction(df, 9, 10, 'Specificity')
```


```{r}
Prec <- confusionFunction(df, 9, 10, 'Precicion')
Prec
ACC=   confusionFunction(df, 9, 10, 'Accuracy') 
ACC
```


# F1 Score

Write a function to calculate the F1 score.

```{r}
f1 <- function(df, actual, predicted){
  f1Tab <- table(df[[actual]], df[[predicted]])
  sens <- confusionFunction(df, actual, predicted, 'Sensitivity') 
  prec <- confusionFunction(df, actual, predicted, 'Precision')
  f1Score <- 2 * sens * prec / (prec + sens)
  return(f1Score)
}

f1(df, 9, 10)
```


# Bounds of F1

The `F1` score is bouned between zero and 1. 

$$
F1_{Score} = \frac{2 * Precicion * Sensitivity}{Precicion + Sensitivity}
$$

For values of a, b $0<a<1$ and $0<b<1$ $ab < a$ and $ab < b$. Therefore the numerator is strictly less than the demominator of the above fraction.


# ROC Curve Function

```{r}
rocFunc <- function(values, predictions){
  # Returns a df of FPR and TPR and a tufte style graph of the AUC.
  # Special thanks too: http://blog.revolutionanalytics.com/2016/08/roc-curves-in-two-lines-of-code.html
  values <- values[order(predictions, decreasing=TRUE)]
  df <- data.frame(TPR=cumsum(values)/sum(values), 
             FPR=cumsum(!values)/sum(!values))
  p <- ggplot(df, aes(FPR, TPR)) +
    geom_line() + 
    ggtitle('AUC Curve') +
    geom_abline(slope = 1) +
    ggthemes::theme_tufte()
  auc <- df %>% 
    mutate(AUC = FPR * lead(FPR) * TPR) %>% 
    select(AUC)
  return(list(auc, p))
}

Temp <- rocFunc(df$class, df$scored.probability)
x <- Temp[1]
data.frame(AUC = matrix(unlist(x))) %>% head() %>% kable()
x  %>% unlist() %>% matrix() %>% data.frame() %>% head() 
Temp[2]
```

# Investigate the caret package

```{r}
library(caret)
confusionMatrix(df$class, df$scored.class)
```

We can see that this is a much more concise way to get many of the values that our function got. Given that it is probably written in C++ it will also be faster. 

# Investigate the pROC package

```{r}
library(pROC)
roc(df$class, df$scored.probability, plot = TRUE)
```

This is also a much more concise way to perform the analysis, however, in my huble opinion, my graph looks better.

There is also the `plotROC` package which performs well:

```{r}
#devtools::install_github("sachsmc/plotROC")
library(plotROC)


ggplot(df, aes(d = class, m = scored.probability)) + 
  geom_roc() + 
  ggtitle('AUC Graph')+
  geom_abline()+
  ggthemes::theme_tufte() 
```

This produces an even better graph. 