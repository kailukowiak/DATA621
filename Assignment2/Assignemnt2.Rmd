---
title: "DATA 621 Assignment 2"
author: "Kai Lukowiak"
date: '2018-03-14'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE) # Necessary to compile to pdf
# Source: https://chrisbeeley.net/?p=1037
```

# Import Data


```{r warning=F}
library(tidyverse)
library(knitr)
df <- read_csv('~/DATA621/Assignments/Assignment2/classification-output-data.csv')
sample_n(df, size = 5) %>% kable()
```


# Confusion Matrix
R's table function can be used to create a confusion matrix. For an more indepth explenation of this please see [this](http://www.saedsayad.com/model_evaluation_c.htm) excelent website. 

```{r}
x <- table(df$scored.class, df$class)
colnames(x) <- c('Actual Positive', 'Actual Negative')
rownames(x) <- c("Predicted Positive", 'Predicted Negative')
x %>% kable()
```

The sum of the rows and columns can give insight into model performance. The rows represent the predicted values while the columns represent the actual values. 

```{r}
# Lable each row for easier computation:
a <- x[1, 1]; b <- x[1, 2]; c <- x[2, 1]; d <- x[2, 2]

Sensitivity <- a / (a + c)
Specificity <- d / (b + d)
PosPredVal <- a / (a + b)
NegPredVal <- d / (c + d)
Accuracy <- (a + d) / (a + b + c +d)
```

* The overall accuracy of the model (`r Accuracy`) shows the total correct classification over all scores. This can be misleading because if 90% of the data is `positive`, a clasifier which only predicts `positive` will be 90% accurate.

* Sensitivity (`r Sensitivity`) of the model is the the ratio of predicted positives to total positives.

* Specificity (`r Specificity`) of the model is the accuracy of negative classification (the opposite of sensitivity).

* Positive and Negative predicted values (`r PosPredVal` and `r NegPredVal` respectively) are positive and negative values that were correctly specified. 


# Accuracy 
Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions.

```{r}
accFunc <- function(df, actual, predicted, metric){
  confMat <- table(df[[actual]], df[[predicted]])
  if (metric == 'accuracy'){
    accuracy <- (confMat[1, 1] + confMat[2, 2]) / sum(confMat)
  return(accuracy)
  }

}

accFunc(df, 9, 10, 'accuracy')
```

# Classification Error Rate

```{r}
accFunc <- function(df, actual, predicted, metric){
  confMat <- table(df[[actual]], df[[predicted]])
  if (metric == 'accuracy'){
    accuracy <- (confMat[1, 1] + confMat[2, 2]) / sum(confMat)
  return(accuracy)
  } else if (metric == 'classError'){
    classError <- (confMat[1, 2] + confMat[2, 1]) / sum(confMat)
    return(classError)
  }
}

accFunc(df, 9, 10, 'classError')
```

To verify that these sum to one:

```{r}
accFunc(df, 9, 10, 'classError') + accFunc(df, 9, 10, 'accuracy')
```

This test is passed.

# Sensitivity

```{r}
accFunc <- function(df, actual, predicted, metric){
  confMat <- table(df[[actual]], df[[predicted]])
  if (metric == 'accuracy'){
    accuracy <- (confMat[1, 1] + confMat[2, 2]) / sum(confMat)
  return(accuracy)
  } else if (metric == 'classError'){
    classError <- (confMat[1, 2] + confMat[2, 1]) / sum(confMat)
    return(classError)
  } else if (metric == 'sensitivity'){
    sensitivity <- confMat[1, 1] / sum(confMat[, 1])
    return(sensitivity)
  } else if (metric == 'precision'){
    precision <-  confMat[[1]] / sum(confMat[1, ])
  }
}
accFunc(df, 9, 10, 'sensitivity')
```

# Precision

```{r}
accFunc <- function(df, actual, predicted, metric){
  confMat <- table(df[[actual]], df[[predicted]])
  if (metric == 'accuracy'){
    accuracy <- (confMat[1, 1] + confMat[2, 2]) / sum(confMat)
  return(accuracy)
  } else if (metric == 'classError'){
    classError <- (confMat[1, 2] + confMat[2, 1]) / sum(confMat)
    return(classError)
  } else if (metric == 'sensitivity'){
    sensitivity <- confMat[1, 1] / sum(confMat[, 1])
    return(sensitivity)
  } else if (metric == 'precision'){
    precision <-  confMat[1,1] / sum(confMat[1, ])
    return(precision)
  }
}

accFunc(df, 9, 10, 'precision')
```

# Specificity

```{r}
accFunc <- function(df, actual, predicted, metric){
  confMat <- table(df[[actual]], df[[predicted]])
  if (metric == 'accuracy'){
    accuracy <- (confMat[1, 1] + confMat[2, 2]) / sum(confMat)
  return(accuracy)
  } else if (metric == 'classError'){
    classError <- (confMat[1, 2] + confMat[2, 1]) / sum(confMat)
    return(classError)
  } else if (metric == 'sensitivity'){
    sensitivity <- confMat[1, 1] / sum(confMat[, 1])
    return(sensitivity)
  } else if (metric == 'precision'){
    precision <-  confMat[1,1] / sum(confMat[1, ])
    return(precision)
  } else if (metric == 'specificity'){
    specificity <- confMat[2,2] / sum(confMat[[2]])
    return(specificity)
  }
}

accFunc(df, 9, 10, 'specificity')
```

# F1 Score

Write a function to calculate the F1 score.

```{r}

```

