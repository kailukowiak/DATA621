---
title: "Assignment 4"
author: "Kai Lukowiak"
date: '2018-03-25'
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
abstract: "This paper looks into the predictive ability of certain factors into the likelyhood of a person getting into an accident and also the amount that the accident will cost."
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r libraries, message=FALSE, warning=FALSE, echo=FALSE}
# Libraries
##################
library(MASS)
library(car)
library(leaps)
library(tidyverse)
library(knitr)
library(kableExtra)
library(psych)
library(ggthemes)
library(corrplot)
library(glmnet)
library(bestglm)
library(xtable)
library(caTools)
options(xtable.floating = FALSE)
options(xtable.timestamp = "")
####################
```
# Data Exploration

## The Data Frames


```{r LoadData, warning=F, message=F, echo=FALSE}
# Loading the data
LabledDF <- read_csv('/Users/kailukowiak/DATA621/Assignments/Assignment3/crime-training-data_modified.csv')

set.seed(101) 
sample = sample.split(LabledDF$zn, SplitRatio = .75)
df <- subset(LabledDF, sample == TRUE)
testDF <- subset(LabledDF, sample == FALSE)
df %>% sample_n(6) %>% kable(caption = 'Sample of Values for the Training Set')
```

The training or labeled data-set is comprised of 12 categorical and continuous variables and one `target` variable that indicates if an area is higher crime.

```{r LoadData2, warning=F, message=F, echo=FALSE}
evalDF <- read_csv('/Users/kailukowiak/DATA621/Assignments/Assignment3/crime-evaluation-data_modified.csv')
evalDF %>% sample_n(6) %>% kable(caption = 'Sample of Values for the Test Set')
###############################
```

The evaluation set is a similar data frame but excludes the target variable. As such it cannot be used for cross validation.

* `zn`: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
* `indus`: proportion of non-retail business acres per suburb (predictor variable)
* `chas`: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
* `nox`: nitrogen oxides concentration (parts per 10 million) (predictor variable)
* `rm`: average number of rooms per dwelling (predictor variable)
* `age`: proportion of owner-occupied units built prior to 1940 (predictor variable)
* `dis`: weighted mean of distances to five Boston employment centers (predictor variable)
* `rad`: index of accessibility to radial highways (predictor variable)
* `tax`: full-value property-tax rate per $10,000 (predictor variable)
* `ptratio`: pupil-teacher ratio by town (predictor variable)
* `lstat`: lower status of the population (percent) (predictor variable)
* `medv`: median value of owner-occupied homes in $1000s (predictor variable)
* `target`: whether the crime rate is above the median crime rate (1) or not (0) (response variable)

## Summary Statistics


```{r, echo=FALSE}
# Summary Tables
SumTab <- summary(df)
SumTab1 <- SumTab[, 1:6]
SumTab2 <- SumTab[, 7:13]
kable(SumTab1, caption = 'Summary Statistics')
kable(SumTab2, caption = 'Summary Statistics')
#####################
```


These tables give an overview of the variables, suggesting there may be some issues with distributions but we will need to look further before making any decisions on transforming the variables.

## Descriptive Statistics

```{r, echo=FALSE}
dis <- describe(df)
dis[, 1:5] %>% kable(caption = 'Descriptive Statistics')
dis[, 6:9] %>% kable(caption = 'Descriptive Statistics')
dis[, 10:13] %>% kable(caption = 'Descriptive Statistics')
```


The count of NA values for each variable is given below.

```{r echo=FALSE}
map(df, ~sum(is.na(.))) %>% t() %>% kable(caption = 'Count of NA Values')
```

There are no missing values.


## Graphical EDA

```{r, echo=FALSE}
df %>% 
  scale() %>% 
  as_tibble() %>% 
  gather() %>% 
  ggplot(aes(x = key, y = value)) +
  theme_tufte() +
  geom_violin()+
  #geom_tufteboxplot(outlier.colour="black")+
  theme(axis.title=element_blank()) +
  ylab('Scaled Values')+
  xlab('Variable')+
  ggtitle('Distrobution of Values', subtitle = 'Y values scaled to fit a common axis')
```



```{r, echo=FALSE}
df %>% 
  scale() %>% 
  as_tibble() %>% 
  gather() %>% 
  ggplot(aes(x = key, y = value)) +
  # geom_violin()+
  # geom_tufteboxplot(outlier.colour="black", outlier.shape = 22)+
  geom_boxplot()+
  theme_tufte() +
  theme(axis.title=element_blank()) +
  ylab('Scaled Values')+
  xlab('Variable')+
  ggtitle('Distrobution of Values', subtitle = 'Y values scaled to fit a common axis')
```


From these two graphs we can see that many of distributions are skewed in one direction or another. It is also interesting to see that the target variable is below zero. This means that the median and mean values are different. 



```{r fig.height=8, echo=FALSE}
ggplot(data = gather(df), mapping = aes(x = value)) + 
  geom_histogram(bins = 10) + 
  facet_wrap(~key, ncol = 2, scales = 'free') +
  theme_tufte() 
```

Similar to the box plots, these histograms show several variables with a lot of skew. In our model selection we will evaluate taking the log of some other skewed, non-binary variables.

```{r, echo=FALSE, echo=FALSE}
corr <- round(cor(df), 1) 
corrplot.mixed(corr, lower.col = 'grey', upper.col = gray.colors(100), tl.col='grey')
```


```{r}
corr2 <- corr
diag(corr2) <- 0 # To remove 1's from variables being correlated with themselves. 
ind <- which(corr2 == max(corr2), arr.ind = TRUE)

maxCorr <- corr[ind][1]
```


This correlation plot shows that while there are some highly correlated variables, the most correlated variable is only `r maxCorr`, which doesn't raise alarm bells WRT multicoliniarity. 

```{r, echo=FALSE}
corrDF <- cor(x = df[, 1:12], y = df$target) %>% 
  as_tibble() %>% 
  rename(Correlation = V1) %>% 
  mutate(VarNames = names(df[, 1:12]))

ggplot(corrDF, aes(x= reorder(VarNames, -abs(Correlation)), y=Correlation)) +
  ggtitle('Correlation of Variables with Target') +
  theme_tufte(base_size=14, ticks=T) +
  geom_bar(width=0.25, fill="gray", stat = "identity") +
  theme(axis.title=element_blank()) +
  scale_y_continuous(breaks=seq(-1, 1, 0.25)) + 
  geom_hline(yintercept=seq(-1, 1, 0.25), col="white", lwd=.3) +
  theme(axis.text = element_text(angle = 45, hjust = 1, colour = 'grey50'))

```

```{r}
interestingCorr <- corrDF %>% filter(Correlation >= 0.5)
```


There are some interesting correlations here. Namely `r interestingCorr$VarNames` all have a correlation over 0.5. The lowest correlation with the target variable is chas.

We will see which variables play more of a role during our logistic classification, but this gives a good preview. 

# Data Preperation

Transformation of variables is less needed for logistic regression because normalcy is not a requirement. However we will transform some variables, which have a large skew, to see if they aid in prediction. 

The other reason to transform variables is to account for interactions between variables. Logistic regression is 'linear' and an exhaustive search of all possible combinations/polynomials would be difficult even if we limited them to three degrees each. Instead, I suggest a test where we fit an extremely non-linear model `KNN` on the data and compare the ROC to the ROC from a simple logistic regression. If there is little difference, it can be safe to assume that the underlying relationship is linear. 

I used [this example](http://rstudio-pubs-static.s3.amazonaws.com/16444_caf85a306d564eb490eebdbaf0072df2.html) to create a KNN model. 


```{r warning=FALSE, message=FALSE, results="hide"}
library(ISLR)
library(caret)
library(ROCR)
library(plotROC)
library(pROC)
set.seed(300)
#Spliting data as training and test set. Using createDataPartition() function from caret
df1 = df
df1$target <- ifelse(df1$target == 1, 'AboveMed', 'BelowMed')
indxTrain <- createDataPartition(y = df1$target,p = 0.75,list = FALSE)
training <- df1[indxTrain,]
testing <- df1[-indxTrain,]

#Checking distibution in origanl data and partitioned data
prop.table(table(training$target)) * 100



trainX <- training[,names(training) != "target"] # Make this target
preProcValues <- preProcess(x = trainX,method = c("center", "scale"))
preProcValues

training$target <- as.factor(training$target)
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 3,classProbs=TRUE,
                     summaryFunction = twoClassSummary)
knnFit <- train(target ~ ., data = training, method = "knn", 
                trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)

#Output of kNN fit
knnAUC <- knnFit$results[1,2]

mod1 <- glm(target~., data = df, family = 'binomial')
prob = predict(mod1,type = c("response"))
g <- roc(target ~ prob, data = df)
logAUC <- g$auc
```

The AUC for the KNN Model is `r knnAUC` and for the logistic regression model it was `r logAUC`. This leads me to believe that there is little need to transformation. 

## Transformed Skewed Variables

The skewed variables `rad` and `tax` have been transformed by taking the log. 
```{r results='asis'}
logDF <- df
logDF$rad <- log(logDF$rad)
logDF$tax <- log(logDF$tax)
logDF$dis <- log(logDF$dis)
# Test Set
logTestDF <- testDF
logTestDF$rad <- log(logTestDF$rad)
logTestDF$tax <- log(logTestDF$tax)
logTestDF$dis <- log(logTestDF$dis)
logDF %>% head() %>% kable()
```

```{r}
dis <- describe(logDF)
dis[, 1:5] %>% kable(caption = 'Descriptive Statistics with Some Transormations')
dis[, 6:9] %>% kable(caption = 'Descriptive Statistics with Some Transormations')
dis[, 10:13] %>% kable(caption = 'Descriptive Statistics with Some Transormations')
```




```{r}
ggplot(data = gather(logDF), mapping = aes(x = value)) + 
  geom_histogram(bins = 10) + 
  facet_wrap(~key, ncol = 2, scales = 'free') +
  theme_tufte() 
```

While I have fixed some of the skewness issues we will need to see if the predictions are better with these.

# Build Models

This project will focus on automated variable selection. New techniques will be compared to the basic logistic regression. 

## Baisic Logistic Regression
The basic logistic regression gives a summary of:


```{r results='asis'}
mod1 <- glm(target ~ ., data = df)
#summary(mod1)
options(xtable.comment = FALSE)
xtable(mod1)
```



Here we have the output of all provided variables without any transformation. 

```{r results='asis'}
logData <- glm(target~., family = binomial(), data = logDF)
xtable(logData)
```


## Stepwise Selection on Baisic Model

If we do a step wise selection to find the variables that limit the scope but still 
provide excellent performance we get:

```{r results='asis'}
step <- stepAIC(mod1, direction="both", trace = FALSE)
xtable(step$anova)
step$coefficients
formulaLength <- length(step$coefficients)
formulaNames <- names(step$coefficients)[2:formulaLength]
stepFormula <- as.formula(paste("target~", paste(formulaNames, collapse="+"))) 
stepFormula 
stepModel <- glm(formula = stepFormula, family = binomial, data = df)
xtable(stepModel)
```

[This](https://www.lexjansen.com/pnwsug/2008/DavidCassell-StoppingStepwise.pdf) presentation offers an interesting critique of step wise selection and some of the issue that make it less ideal. 

## BestGLM Model Selection
This model selection using the steps algorithm selects significantly fewer variables.
```{r}
df1 <- df
df1 <- dplyr::rename(df1, y = target)
df1$y <- as.factor(df1$y)
df1 <- data.frame(df1)
BestGLMModel <- bestglm(df1, family = binomial)
#xtable(BestGLMModel)
BestGLMModel
```

## LASSO Regression

Looking at lasso logistic regression might give us a better model selection and coefficient values. Below is the results. 

```{r}
X <- df %>% dplyr::select(-target)
X <- data.matrix(X)
#X <- as.matrix(X, ncol=12)
y <- as.factor(df$target)
fit = glmnet(X, y, family = "binomial")
library(ROCR)
aucDF <- X
lasso.model = cv.glmnet(X, y, family = "binomial", type.measure = 'class')

aucDF$lasso.prob <- predict(lasso.model, type="response", newx = X, s = 'lambda.1se')
pred <- prediction(aucDF$lasso.prob, y)


cvfit = cv.glmnet(X, y, family = "binomial", type.measure = "class")
plot(cvfit)
coef(cvfit, s = "lambda.1se")
```

## Regular logostic without LASSO Dropped Variable

This models coefficients deviate significantly from a normal `glm` model that excludes the one variable dropped. This is because LASSO penalizes large coefficients. For example, `glm` model excluding `rm` is:

```{r results='asis'}
mod3 <- glm(target ~ . -rm, family = 'binomial', data = df)
xtable(mod3)
```

This is interesting because we can see how different the coefficients are even though it has the same variables.



## Lasso with scaled variable

```{r}
logX <- logDF %>% dplyr::select(-target)
logX <- data.matrix(logX)
y <- as.factor(logDF$target)

# Testing Data
logXTest <- logTestDF %>% dplyr::select(-target)
logXTest <- as.matrix(logXTest)
yTest <- as.factor(logTestDF$target)

# Fitting
fit = glmnet(logX, y, family = "binomial")
library(ROCR)
aucDF <- logX
lasso.model = cv.glmnet(logX, y, family = "binomial", type.measure = 'class')

aucDF$lasso.prob <- predict(lasso.model, type="response", newx = logX, s = 'lambda.1se')
predScaled <- prediction(aucDF$lasso.prob, y)


cvfitLOG = cv.glmnet(logX, y, family = "binomial", type.measure = "class")
plot(cvfitLOG)
coef(cvfitLOG, s = "lambda.1se")
```

# Chose a Model

Model selection in the previous section used a variety of different methods ranging from none to BIC to AIC. As such it is unfair to select a criteria that we have used already. 

I've chosen to use AUC because it is easily understood and provides a nice visual way to differentiate model performance.

## Baisic GLM Model

```{r}
AUC <- function(testDF, mod, modelName){
  library(plotROC)
  library(pROC)
  prob = predict(mod,type = c("response"))
  df$prob=prob
  p = ggplot(df, aes(d = target, m = prob)) + 
    geom_roc(n.cuts = 0) + 
    ggtitle(paste('AUC Graph for', modelName)) +
    xlab("False Positive Fraction") +
    ylab('True Positive Fraction') +
    geom_abline(linetype = 'dashed') +
    theme_tufte()
  
  g <- roc(target ~ prob, data = df)
  return(list(p, g$auc))
}
```

We see this model has good performance, especially for a data set that has roughly equal numbers of positive and negative examples. 

## GLM Model Evaluation 

```{r}
AUC(testDF, mod1, 'Baisic GLM Model')
```

## Scaled GLM Model

```{r}
AUC(logTestDF, logData, 'Sclaed GLM')
```


## STEPWISE AIC MODEL

```{r}
AUC(testDF, step, 'Step AIC Model')

```

## BestGLM Model

```{r}
AUC(testDF, BestGLMModel$BestModel, 'Model Best')
```

## Lasso Model 
```{r}
#testX <- a
fittedGLMcv <- predict(cvfit, X, s = "lambda.1se", type = "class")

perf <- performance(pred,"tpr","fpr")
auc <-  performance(pred,"auc") # shows calculated AUC for model
auc <- auc@y.values

roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values))

ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
    #geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    geom_abline(slope=1, intercept=0, linetype='dashed') +
    ggtitle("ROC Curve") +
    ylab('True Positive Rate') +
    xlab('False Positive Rate') + theme_tufte()
auc
```

## Scaled LASSO

```{r}
fittedGLMcvLog <- predict(cvfitLOG, logXTest, s = "lambda.1se", type = "class")

perf <- performance(predScaled,"tpr","fpr")
auc <-  performance(predScaled,"auc") # shows calculated AUC for model
auc <- auc@y.values

roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values))

ggplot(roc.data, aes(x=fpr, ymin = 0, ymax=tpr)) +
    #geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    geom_abline(slope=1, intercept = 0, linetype='dashed') +
    ggtitle("ROC Curve") +
    ylab('True Positive Rate') +
    xlab('False Positive Rate') + theme_tufte()
auc
```


## Which model?

All models performed well with no model below 0.95. The best model was the simple scaled logistic regression model. Making the variables more normal increased the fit. One point of interest is that the LASSO model performed the best without a scale, so if there were an theoretical reasons that scaling a variable was not desireable, we could use that. 

## Prediction
The predictions for the test set are given below. The first column corresponds to the probabilities and the second column corresponds to the actual prediction (the rounded probabilities).

Surprisingly, the simplest scaled model worked best. While models like LASSO performed better than the non-scaled values, scaling some variables led to the best performance with a simple logistic regression.

```{r}
testMat <- data.matrix(evalDF)
PredictedProbabilities <- predict(logData,type = "response", newx =  testMat) 
PredictedValues <- round(PredictedProbabilities)
predDF <- data.frame(PredictedProbabilities, PredictedValues)
colnames(predDF) <- c('Predicted_Probabilities', 'Predicted_Outcome')
predDF %>% head() %>% kable()
```

# Apendix

```{r appendix, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
``` 

